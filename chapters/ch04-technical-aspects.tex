% !TEX root = ../thesis.tex

\chapter{Fast polyhedra analysis with deep Q-networks}
\mbox{}\\
The goal of this work was to build upon the existing solutions used to make polyhedra analysis more efficient and use deep Q-networks to further improve the performance. Specifically, the goal was to extend the Elina Framework \cite{elina}, the version of it that uses reinforcement learning for polyhedra analysis, in order to replace the Q-learning method with deep Q-networks for the Q-function estimation. Our work can be separated into three major subtasks. First of all, it was necessary to figure out a way of calling a deep neural network from Elina, so that it could be used for the Q-function estimation. Secondly, In order to test out the validity of deep Q-networks on polyhedra analysis and to have a baseline for further optimisation. We decided to implement the basic deep Q-network training algorithm. Finally, we attempted to further optimise this problem with some task specific knowledge.

\section{Incorporating Neural Networks inside Elina}
The first part, whilst definitely being the least interesting was probably the most laborious and a very key part of the whole work. The problem was that for the Q-value estimation we wanted to use neural network regression from the Keras framework in Python. We decided to use this framework as it is very widely used for this type of problem, is relatively easy and intuitive to use whilst remaining very powerful. However, Elina being written in C, the link between the python and C code had to be made. This was finally achieved  by using the Python/C API and embedding the python code into the Elina framework. Initialising all the python code as well as importing all the necessary libraries such as Keras, is done in the $op\_pk\_manager\_alloc()$ function so that it does not have to be done every time the join function is called. Inside the join function we simply have to call the learning or prediction algorithms.\\
One advantage of such a modification to the framework, is that we can now learn online. In comparison to \cite{singh2018fast} where the training was done offline. Online training allows the algorithm to pick the best action according to the current decision policy. This is a very important feature for deep Q-networks, as it allows us to set an exploration rate and to reinforce the already learned policy, this is vital to prevent divergence.
\section{Basic algorithm}
In order to get the basic training algorithm working, first the domain of polyhedra analysis has to be made compatible with the domain of reinforcement learning. Therefore the following list of items have to be initialised inside of the polyhedra domain:
\begin{itemize}
	\item A set of features describing the polyhedra to use as states $S$
	\item A set of actions $A$
	\item A reward function $r$
\end{itemize}
Luckily, since a reinforcement learning method \cite{singh2018fast} has already been used for polyhedra analysis we can simply reuse what has already been developed for our baseline algorithm.\\
\subsection{Features}
The reinforcement learning method uses a set of nine different features.
\begin{itemize}
	\item The number of blocks.
	\item The minimal, maximal and average size of a block.
	\item The minimal, maximal and average size of the generator set off the union of input factors corresponding to a block.
	\item The number of variables with finite upper and lower bound.
	\item The number of variables with one finite and one infinite bound.
\end{itemize}
We will start off by reusing the same features. However we will modify their bucketing, since, due to the nature of the Q-learning method, it is very restrictive. The new bucketing will be discussed in section 4.3.2.

\subsection{Actions}
As we discussed in 3.3.1., the bottleneck of the decomposed analysis is the join. Therefore, the set of actions will be composed of various joins of different precision and performance.\\
First of all, the cost of the joins depends on the size of the block. Therefore, bounding the the size of the block with a threshold would increase the performance. These four different thresholds are used $threshold \in [5,9],[10,14],[15,19],[20,\infty)$\\
Once we have decided on the size of a threshold we have to equally decide on what to do if the block has a greater size than the threshold. Their are different possibilities of how to split a large block into smaller ones, but basically, one has to pick a subset of constraints to remove from the block so as to make some of the variables no longer dependant and then it can be further partitioned. The following three constraint removals are used:
\paragraph{Stoer-Wagner min-cut}\mbox{}\\
This algorithm is based on the simple idea of removing the minimal amount of constraints in order to be able to split the block into two separate permissible partitions \cite{stoer1997simple}.
\paragraph{Weighted constraint removal}\mbox{}\\
The second and third constraint removal techniques are based on the same principal. They associate a certain weight to each constraint and then remove the constraint with the highest weight. Two different weight distribution techniques are considered. \\
In the first one, we first compute for each variable $x_i \in X$ the number of constraints it appears in, we can call this $n_i$. Then for each constraint $c_i$ we set its weight to the sum of $n_i$ of all the variables that are in the constraint.\\
The second method, we first compute for each pair of variables $x_i,x_j\in X$, $n_{ij}$ the number of constraints containing both $x_i$ and $x_j$. The weight of the constraint is then the sum of all the $n_{ij}$ of all the pairs of variables $x_i,x_j$ contained in the constraint.\\
The idea behind removing the constraint with maximal weight, is that, most likely the variables with a large weight also occur in other constraints and therefore will not become unbounded once the constraint is removed. In this way we retain precision.
\paragraph{Merging blocks}\mbox{}\\
The next three actions are various block merging strategies. The idea is to select different blocks and merge them together as long as the resulting blocks remain below the threshold in order to increase the precision of the subsequent join. The following three block merging strategies are used:
\begin{itemize}
	\item No merge, no blocks are merged
	\item Merge smallest first, we first merge the smallest two blocks together. We then remove the smallest block and continue as long as the resulting merge remains below the threshold.
	\item Merge small with large, similar to the previous strategy but this time me merge the smallest block with the largest.
\end{itemize}

In total we have four different thresholds, three different constraint removal algorithms and three different block merging strategies. We can mix and match these together as we please, which means that in total we have $4\cdot 3\cdot 3 =36$ different actions we can pick.

\subsection{Reward}
As a reminder, the objective of the reward is to guide our learning policy, rewarding it when it takes actions towards our global goal and penalising it when it does otherwise. Therefore, the reward developed by the Q-learning method was the following:
\begin{equation}
	r(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb} - \log_{10}(cyc)
\end{equation}
Where $n_s$ is the number of exactly defined variables of the resulting polyhedron after the join. $n_b$ is the number of bounded variables and $n_{hb}$ the number of semi-bounded variables of the resulting polyhedron. $cyc$ is the number of cycles required to perform the join.
\subsection{Back to deep Q-networks}
As we have finally initialised polyhedra analysis for the use with reinforcement learning. We can finally proceed to the more interesting part of designing the training algorithm. Under its most basic form, the training algorithm could simply be the following. Pick a random reward or maximal action, observe the reward and then retrain the network with the observed reward plus the discounted future reward on the given action. This very basic algorithm however does not work very well as it runs into two major complications.

\paragraph{Divergence of decision policy}\mbox{}\\
The first of which is the divergence of the action selection strategy. Meaning that the neural networks were not able of creating a consistent strategy but would be picking different actions almost randomly all the time.
\paragraph{Divergence of the action value prediction}\mbox{}\\
The second problem was that the predictions of the estimators would diverge towards infinity. This is also known as the exploding gradient problem.\\
\mbox{}\\
These problems are not new and are a fundamental problem of trying to use non-linear functions for the Q-function approximation. Several techniques exist to combat these problems. Many of which reduce the complexity of the network and/or the problem in general. However, some new methods have been developed recently that would allow the training of the estimators whilst retaining the problems original complexity. We decided the use the following two concepts inside of the algorithm.
\subsection{Experience replay memory}
Experience replay memory \cite{Mnih2015} is a biologically inspired mechanism. That gives the estimator a very basic concept of a memory and instead of directly learning from the current events happening. The agent learns from a random subset of its memory. More formally, during training, an array of a certain size filled with the past memory objects is kept. Each memory item contains the following items: $mem(s_t, a_t, r_t, s_{t+1})$. The current state, the action taken at this state, the observed reward and the next state. For training we then simply pick a random subsample from the memory array and train from these examples.
\\
The objective of the memory is to homogenise the training data. It comes from the observation that during the execution of a program, often a particular strategy would be optimal for a certain period of time and afterwards another one would be the new optimal. This causes two major problems. First of all, it is not very time efficient as we do not gain much information by learning from the same data. Secondly, as there is low diversity in the training data and the decision strategy would frequently abruptly change. This either leads to the neural network getting stuck in local minima or simply to diverge and not obtain a policy. \\
Picking a random subsample from memory helps increase the variance amongst the training data allowing the network to learn a more global policy. 

\subsection{Separating target from max Q estimators}
The other method used to reduce the divergence of the predicted Q-values towards infinity, was to reduce the correlation between the training and the prediction data \cite{Mnih2015}. This was done because, since the networks are being fitted partly on the maximum Q-value estimation, diverging towards infinity reduces their error and therefore is a valid strategy that the networks can exploit. In order to reduce this correlation, the networks predicting the maximum Q-value and the ones predicting the Q-value can be separated. The weights of the maximum Q-value estimators are then updated every n steps in order for them to remain up to date.\\
\mbox{}\\
Further modifications were also made on the neural network level in order to reduce these problems. We will get into these in part 4.4.\\
Once these modifications have been made the basic training algorithm has the following form.
\paragraph{Pseudo code of basic learning algorithm}

\begin{center}
	
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{learn} $(S,A,r,\gamma,\alpha,\phi,N,l\_freq,b\_size,update\_nn\_freq,\varepsilon)$\;
    
    
    \Indp\Input{\\$S \leftarrow states, A \leftarrow Actions, r \leftarrow reward,$\\$ \gamma \leftarrow discount$ $factor, \alpha \leftarrow learning$ $rate,$\\$\phi \leftarrow$ set of feature functions over S and A \\ N $\leftarrow$ size of memory, l\_freq$\leftarrow$ learning frequency,\\ b\_size $\leftarrow$ batch size, $\varepsilon \leftarrow$ random action probability,\\ update\_nn\_freq $\leftarrow$ frequency of updating max Q estimators}\Indm
    \Indp\Output{\\$\theta \leftarrow$ trained weights of the estimator}\Indm
    \Indp
    $\theta = $ initialise random weights and learning rate $\alpha$\\
    $\theta_{max} = $ initialise random weights and learning rate $\alpha$\\
    $M = $ initialise an empty memory with size N\\
    
    \For{each episode}{
    start from initial state $s_{_0} \in S$\\
    \For{$t=0,1,2,...$}{
    Initialise new memory item $m_t$\\
    With probability $\varepsilon$ take random or maximal action $a_t$\\
	observe next state $s_{t+1}$ and
     $r_{t}(s_{t},a_t,s_{t+1})$\\
     Set $m_t.a = a_t,m_t.s_1=s_{t},m_t.s_{2}=s_{t+1},m_t.r=r_{t}$ \\
     
     \Indm
     \Indp\uIf{$M_{size} \geq N$}{
    	del $M_0$ \\
     	$M_{N}=m_t$
		}
		\uElse{
		push $m_t$ on M
	}\Indm
	\Indp\uIf{$t\mod l\_freq = 0$}{
	select a random batch of size b\_size from m\\
	compute $Q(:,s_t)$ estimation with $\theta$\\
	compute $Q(:,s_{t+1})$ estimation with $\theta_{max}$\\
	set $Q(a,s_t) = Q(a,s_t) + \gamma*max(Q(:,s_{t+1}))$\\
	Fit weights $\theta$ with new training data
	}\Indm
	\Indp\uIf{$t\mod update\_nn\_freq = 0$}{
	set $\theta_{max} = \theta$
	}
			
    }
    
   }
   \textbf{return} $\theta$
    
\caption{DQN Training algorithm}
\end{algorithm}
\end{center}
\mbox{}\\
\mbox{}\\
\mbox{}\\

\section{Further optimisations}
Once the basic version of the deep Q-network algorithm was designed we were able to train it, test it and obtain our first set of results. The discussion of these will be done in chapter 5. These results can from now on be used as a baseline and we will attempt to further improve our results.\\
In the search of a more efficient algorithm, we decided to incorporate some problem specific knowledge to the deep Q-network algorithm. As well as further optimisations to the feature vector and reward functions. The modifications made will be discussed in the rest of this section.


\subsection{Separating the problem into two}
The first problem specific modification was subdividing the problem into two independent subproblems. The objective of Polyhedra Analysis is to obtain the most precise result in the quickest way possible. These are two independent objectives. The goal was therefore to separate these two tasks and create two independent subsystems, one focusing on making the results as precise as possible, and the other on the time complexity of getting there.\\
 Such a division of the problem, offers two main advantages. Firstly, a greater flexibility during training. The parameters of each system can be tuned for its specific needs. So for example, the characteristics of the neural network or the discount factor. Furthermore, the optimal features used for precision and performance estimation, are most likely different. The separation therefore allows us to use only the necessary features for each subsystem making the learning and prediction less complex and therefore more precise and converge faster. The reward as well can be fine tuned for the needs of the specific subsystem.\\ 
 Secondly, two separate subsystems also allow for a more flexible algorithm post training. Since during training, we will have  two different Q-function estimators, during prediction we will have a greater possibility of optimising our results by changing the importance we give to each subsystem at different points in time according to the specific needs. We will get into more detail about this in the section 4.3.4.\\
 The pseudo code for the training algorithm with separated estimators has the following form:


\paragraph{Pseudo code of the separated training algorithm}
\begin{center}
	
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{learn} $(S,A,r,\gamma,\alpha,\phi,len,l\_freq,b\_size,update\_nn\_freq)$\;
    
    
    \Indp\Input{\\$S \leftarrow states, A \leftarrow Actions, r \leftarrow reward,$\\$ \gamma \leftarrow discount$ $factor, \alpha \leftarrow learning$ $rate,$\\$\phi \leftarrow$ set of feature functions over S and A \\ len $\leftarrow$ size of memory, l\_freq$\leftarrow$ learning frequency,\\ b\_size $\leftarrow$ batch size,\\ update\_nn\_freq $\leftarrow$ frequency of updating max Q estimators}\Indm
    \Indp\Output{\\$\theta_1 \leftarrow$ trained weights of neural network for performance \\ $\theta_2 \leftarrow$ trained weights of neural network for precision}\Indm
    \Indp
    $\theta_1 = $ initialise random weights\\
    $\theta_2 = $ initialise random weights\\
    $\theta_{1\_max} = $ initialise random weights\\
    $\theta_{2\_max} = $ initialise random weights\\
    $m = $ initialise an empty memory\\
    
    \For{each episode}{
    start with initial states $s_{pr\_0} \in S, s_{pe\_0} \in S$\\
    \For{$t=0,1,2,...$}{
    Initialise new memory item $m_t$\\
    Take action $a_t$ according to the action selection algorithm \\ observe next state $s_{pr\_t+1},s_{pe\_t+1}$ and
     $r_{pe}(s_{pe\_t},a_t,s_{pe\_t+1}),r_{pr}(s_{pr\_t},a_t,s_{pr\_t+1})$\\
     Set $m_t.a = a_t,m_t.s_{pr\_1}=s_{pr\_t},m_t.s_{pe\_1}=s_{pe\_t},m_t.s_{pr\_2}=s_{pr\_t+1},m_t.s_{pe\_2}=s_{pe\_t+1},m_t.r_{pr}=r_{pr},m_t.r_{pe}=r_{pe}$ \\
     
     \Indm
     \Indp\uIf{$m_{size} \geq len$}{
    	del $m_0$ \\
     	$m_{len}=m_t$
		}
		\uElse{
		push $m_t$ on m
	}\Indm
	\Indp\uIf{$t\mod l\_freq = 0$}{
	select a random batch of size b\_size from m\\
	compute $Q(:,s_t)$ estimation with $\theta_1,\theta_2$\\
	compute $Q(:,s_{t+1})$ estimation with $\theta_{1\_max},\theta_{2\_max}$\\
	set $Q(a,s_t) = Q(a,s_t) + \gamma*max(Q(:,s_{t+1}))$\\
	Fit weights $\theta_1,\theta_2$ with new computations from this batch
	}\Indm
	\Indp\uIf{$t\mod update\_nn\_freq = 0$}{
	set $\theta_{1\_max} = \theta_1,\theta_{2\_max}= \theta_2$
	}
			
    }
    
   }
   \textbf{return} $\theta_1,\theta_2$
    
\caption{DQN Training algorithm}
\end{algorithm}
\end{center}



\subsection{Feature selection}
Before we get into the specific features that were used for the estimators, lets go a bit more in-depth about the theory of features and states. In the case of polyhedra analysis states would be the concrete Polyhedra. However, during our analysis we do not use the concrete polyhedra as this would be too complicated, but rather some information that we extract from them, we collect this information inside of the feature vector. As explained in section 2.1., for reinforcement learning, states should respect the Markov property, which states that state $s_{t+1}$ only depends on $s_t$ and on $a_t$. Whilst this is the case for the polyhedra, our feature vectors do not necessarily obey this property as they do not represent the polyhedra themselves but only some information about them. The more precisely we manage to describe our polyhedron the closer we will be to respecting the Markov property and the better will be the results of reinforcement learning. 

With regards to the old set of features. One big inconvenience of using Q-learning, is that, since we want to represent our Q-function with basis functions, the size and dimensions of our feature vector is very limited. This constraint no longer holds when using more advanced methods such as deep Q-networks. This allowed us to make two major changes about the feature vector. 
\paragraph{Adding new features}\mbox{}\\
The first change we made, was adding some new features.First of all, we reused the nine features from the Q-learning algorithm. As a reminder these features are the following: the first seven features are used to characterise the complexity of the join. They are the number of blocks, minimal, maximal and average size of the blocks and the minimal, maximal and average size of the generator set. The last two features are used to characterise the precision of the inputs and they are the number of variables with a finite upper and lower bound, as well as the number of variables with a finite upper or lower bound, in both Polyhedra.\\
We further extended this set with four new features, in order to further increase the description accuracy of the feature vector. The selection of these features was done by trial and error, with an experimental observation of an increase, or lack thereof, of accuracy. It is also possible to check whether a particular feature is useful once training is finished by analysing the neural network and observing the impact a particular feature has on the end result. However, since at the end our network has a total of four layers, this made its analysis somewhat complicated.\\
At the end, we decided to add a total of four new features, three of which are used for modelling the precision and one for the complexity. For the complexity, we added the number of variables. As for the precision, we added the number of unconstrained variables, the number of exactly constrained variables and finally, the sum of the values the bounded variables can have (i.e. an approximation of the circumference of the polyhedra). We also attempted using some features that would have perhaps modelled the precision more accurately, such as most notably approximating the volume of the polyhedra. Unfortunately, one must also consider the complexity of computing the features. The computation of the volume approximation was far too time complex, which greatly increased the learning time making the feature not viable.\\
\paragraph{Bucketing}\mbox{}\\
Due to the limitations of Q-learning the old algorithm uses a very restrictive bucketing policy. Deep reinforcement learning does not suffer from such restrictions. It was therefore possible for us to remove the bucketing from the feature selection. We still decided to implement a version of bucketing inside of the algorithm for three main reasons. \\
Firstly, for features with very big values an exact precision is not needed. For example, for the feature that approximates the circumference of the polyhedra, its total value can be very big and and if it is a million and one or just a million doesn't impact the resulting precision much, therefore bucketing does not impact it either. \\
The second reason, for the use of bucketing is that it greatly decreases the learning time and helps convergence. \\
Finally, the last reason is, so that the decision policy does not give more importance to some features simply because they are larger in value than others. In total we have thirteen different features and they can all have very different values. For example, the number of blocks varies mainly between one and ten and the circumference of the polyhedra can go up to $10^9$. This does not mean that the circumference has a greater impact on the overall description of the polyhedron. Bucketing assures that all features have a similar impact on the decision policy.\\
In order to decide the values of the bucketing that would be used,  the algorithm was run on a big number of benchmarks computing the maximal, minimal and average values the different features could have. Finally, they were scaled in such a way, so that they would all approximately remain between the bounds of zero and ten. The resulting bucketing has the following form:
\begin{center}

\Indm\Indm\begin{tabular}{||c c c c||} 
 
 \hline
 Feature & Extraction complexity & Approximate range & Scaling \\ [0.5ex] 
 \hline\hline
 $|\beta|$ & $O(1)$ & 1-10 & $x/1.$ \\ 
 \hline
 $min(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $max(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $avg(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $min(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $max(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $avg(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $\{|x_i \in X: x_i \in (-\infty,\infty)$ in $P_m|\}$ & $O(ng)$ & 1-100 & $round((x/10),0.5)$ \\ 
  \hline
 $\{|x_i \in X: x_i \in [l_m,\infty)$ in $P_m|\} + $ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
 $ \{|x_i \in X: x_i \in (-\infty,u_m]$ in $P_m|\} $ & & & \\
 \hline 
 $\{|x_i \in X: x_i \in [l_m,u_m]$ in $P_m|\}$ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
   \hline
 $\{|x_i \in X: x_i \in [u_m,u_m]$ in $P_m|\}$ & $O(1)$ & 1-200 & $round((x/20),0.2)$ \\ 
    \hline
 $|X|$ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
    \hline
 $\sum (u_m-l_m):u_m,l_m \in\{[l_m,u_m]$ in $P_m \}$ & $O(ng)$ & $2*10^9$ & $round((x/2*10^8),0.01)$ \\ 
 
 
 \hline
\end{tabular}
\end{center}
One thing to note is, that opposed to the reinforcement learning algorithm, these features do not have a maximal possible value. We believe that this increases the precision of the Q-function estimation, especially for the approximation of the complexity. Certain features can have very large values and we believe that when this arrives, it has a major impact on the complexity of the join. However, these very large values were ignored by the bucketing of the Q-learning algorithm. 

\subsection{Reward Function modelling}
Due to the nature of the new algorithm, the modelling of the reward function was separated into two separate subproblems.
\paragraph{Performance reward}\mbox{}\\
Firstly, the reward for the precision estimator and then the reward for the performance estimator. Modelling the complexity is fairly simple and very straight forward, as simply counting the number of cycles needed for the computer to execute the join perfectly models the joins complexity and is exactly what we want to optimise. One thing to note is that we want this reward to be as small as possible, two simple solutions for this are either to invert it or to negate it. After some experimentation with both of these we decided that negating it produces better results. This is probably due to the fact, that inverting the reward makes it have a nonlinear curve and its derivative looses importance the higher the CPU cycles are, which is not something we want to model. Another thing to note is that modelling the complexity reward in this way give us another advantage over the reinforcement learning version of the algorithm. The Q-learning algorithm took the $\log_{10}$ off the CPU Cycles in order for the complexity and the precision reward to have similar values. We believe that this produces a similar problem as inverting the reward. The reward is no longer linear and its differences loose importance the higher it gets. Once again, this is not something that we want to model. The final rewards has the following form:
\begin{equation}
	r_{pe}(s_t,a_t,s_{t+1}) = -1 \cdot cyc
\end{equation}
Where $cyc$ is the number of CPU cycles needed to perform the join.
\paragraph{Precision reward}\mbox{}\\
As to the second reward, modelling the precision of the resulting join. This is considerably more difficult than modelling the complexity as there is no trivial element giving us the precision of our polyhedron. In order to choose the reward, we proceeded by intuitively picking a small set of options and verifying them experimentally at the end. At the end, we tested out three different reward.\\
We took the first one from the reinforcement learning algorithm, that is, its objective is to maximise the amount of exactly bounded, bounded and half bounded variables. The second is a further extension by penalising the amount of values a bounded variable can have in the resulting Polyhedra. The final, reward function is slightly different. In this one, we penalise the loss of an exactly bounded, bounded or half bounded variable. Reward the loss of a unbounded variable and penalise the amount of values a bounded variable can have. We also tried basing a reward function on an approximation of the volume of the resulting Polyhedra, unfortunately, the complexity of this computation was too high which made it too impractical to use. The final rewards have the following form:
\begin{equation}
		r_{pr_1}(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb}
\end{equation}
\begin{equation}
		r_{pr_2}(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb} - \log_{10}(|n_b|)
\end{equation}
\begin{equation}
		r_{pr_3}(s_t,a_t,s_{t+1}) = 3  \cdot (n_{s_f} - n_{s_i}) + 2 \cdot (n_{b_f} - n_{b_i}) + (n_{hb_f} - n_{hb_i}) - \log_{10}(|n_b|)
\end{equation}\mbox{}\\
Where $n_s$ is the number of exactly defined variables of the resulting polyhedron after the join. $n_b$ is the number of bounded variables and $n_{hb}$ the number of semi-bounded variables of the resulting polyhedron. $n_{x_f}$ is the number of variables after the join and $n_{x_i}$ before the join.
\subsection{Action selection algorithms}
As discussed before, the Q-function estimation was separated into two independent subproblems. As we have already mentioned earlier, this allows a certain amount of benefits that would not be possible otherwise. However, it also imposes one major complication. These two subproblems are not totally separable, as once we have predicted the two Q-function estimations, we have to somehow merge the information contained in both of these estimations and pick an ideal action accordingly. We tried out a few different action selection algorithms in order to find the one that maximises precision and performance the most. One thing to note is that it is at least partially possible to test out these algorithms post training. That is to say that we do not have to train using these in order to measure their performance afterwards. This only works if we train purely randomly however. As if, if we were to train using one selection algorithm and then test with another, this would surely deteriorate the results for the other algorithm. The fact that we are able to test the selection algorithms after random training is still very helpful as the training time is relatively high and having to train for each algorithm would be very time intensive.
\paragraph{Algorithm 1}\mbox{}\\
The first selection algorithm we used is perhaps the most intuitive one. First scaling both predictions. The performance one between [-1,0] and the precision one between [0,1]. Adding the values together and picking the action with the maximal value. Whilst seeming very fair this type of selection has a couple of fundamental flaws. First of all, reinforcement learning estimates the best action to take in order to maximise the long-term objective, it however has less of a guarantee about the second to best and third to best action. This means that if the network is well trained, the chances that the action with the maximal Q-value prediction is also the best action to take at this point in time should be quite high. However, the guarantee that the action with the second highest Q-value prediction is the second-best action to take is much smaller. Using this selection algorithm tends to quite often not choose the best action but the second best or the third etc... making the probability that they are also good actions also a lot smaller. The second problem with this type of selection algorithm is that scaling the reward function causes a loss of information that could be very important. Lets demonstrate this with an example, let's say we have four joins j1,j2,j3,j4. J1 takes one second, j2 ten seconds, j3 one hour and j4 ten hours. Let's also say that j1 and more precise than j2 and j3 is more precise than j4. This algorithm is going to treat the difference between j1 and j2 the same as the one between j3 and j4, even though the gain in precision might be worth to loss in performance for the case of j1 and j2, whilst this would probably not be case for j3 and j4.
\paragraph{Pseudo code of algorithm 1}
\begin{center}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select1} $(Q_{pr},Q_{pe})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    $Q_{pr} = Q_{pr}/max(Q_{pr})$\\
    $Q_{pe} = Q_{pe}/min(Q_{pe})$\\
    $a = max_{arg}(Q_{pr}+Q_{pe})$
    
  
   \textbf{return} $a$

\caption{Action selection algorithm 1}
\end{algorithm}
\end{center}
\paragraph{Algorithm 2}\mbox{}\\
The second algorithm used, that was designed to confront both problems of the previous algorithm, proceeds as follows. We set some sort of threshold for the performance. We then look at the action that has the maximal Q-value for precision. If this actions Q-value for performance is above the threshold we take it otherwise we take the second-best action and continue until we find an action that is above the threshold. Again, this algorithm seems quite good at first glance and maybe if it was executed perfectly it would be the best option, but like the one before it has some fundamental problems. First of all, picking a threshold is not a very simple task. The predictions are done with regression neural networks, the activation function of there last layers are linear, this means that the output values are unbounded and they do not have a direct correlation with Realtime CPU cycles. In order to pick a threshold, we had to experimentally try different possible values and observe the resulting precision and adjust accordingly, however this threshold would vary according to the benchmark and therefore choosing an optimal one was a very difficult task on its own. Another problem of this algorithm is that if no action has a Q-performance estimation above the threshold the algorithm will choose the action with the worst precision, and this one does not even have to have the best performance estimation. 
\paragraph{Pseudo code of algorithm 2}
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select2} $(Q_{pr},Q_{pe},PE_{thresh})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    \While{$max(Q_{pr})\neq 0$}{
    a = $max_{arg}(Q_{pr})$\\
    \uIf{$Q_{pe}(a)\geq PE_{thresh}$}{
    break;
    }
    $Q_{pr}(a)=0$
    }
    
   
  
   \textbf{return} $a$
   
    
\caption{Action selection algorithm 2}
\end{algorithm}
\end{center}
\paragraph{Algorithm 3}\mbox{}\\
Another possibility is to slightly modify the previous algorithm in order to minimise some of its short comings. This time we set both some sort of a threshold for the performance and a certain number x. if the action that has the highest precision estimation is under the threshold for performance, we take the x best actions according to their precision and take the one that has the highest performance estimation. The problem of picking an appropriate threshold remains the same and this time we have the further problem of picking a correct value for x. However, the case that all actions are under the threshold is no longer a problem. It is worth noting that this algorithm also has a greater time complexity, however this is still greatly outweighed if the correct action is taken.
\paragraph{Pseudo code algorithm 3}
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select3} $(Q_{pr},Q_{pe},PE_{thresh},N_{act})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold, \\$N_{act}\leftarrow$number of actions to consider if below threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
   	$a=max_{arg}(Q_{pr})$\\
   	\uIf{$Q_{pe}(a)\leq PE_{thresh}$}{
   		$a_{max}=a$\\
   		$val_{max}=Q_{pr}(a)$\\
   		$Q_{pr}(a)=0$\\
   		\For{$i \in N_{act}$}{
		   	$a=max_{arg}(Q_{pr})$\\
		   	\uIf{$Q_{pe}(a)\geq val_{max}$}{
		   		$a_{max}=a$\\
		   		$val_{max} = Q_{pe}(a)$
		   	}
		   	$Q_{pr}(a) = 0$
   		}
   	}
    
   
  
   \textbf{return} $a_{max}$
   
    
\caption{Action selection algorithm 3}
\end{algorithm}
\end{center}
\paragraph{Algorithm 4}\mbox{}\\
Finally, the last selection algorithm that we will present here is based upon the previous one, but modified in such a way as to reduce the importance of correct parameter choosing. This time we begin by scaling the performance prediction to [-1,0]. We set a threshold between [0,1]. We pick the action that has the highest Q-precision estimation. If the absolute value of this action's performance estimation minus the maximal performance estimation is below the threshold, then we pick this action otherwise we pick the second-best precision action until we find one that is below the threshold. This time, since we compare to the best performance action we are guaranteed to be below the threshold at some point, therefore, in the worst case we will pick the action with the best performance. The threshold is also a lot easier to set since it is bounded. Unfortunately, once again we have the problem caused by the scaling of the performance estimation. However, after some experimental observation, we observed that when the join is fast all the Q-performance estimations tend to be quite close together, so hopefully this should not be all too much of a problem.
\paragraph{Pseudo code algorithm 4}
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select4} $(Q_{pr},Q_{pe},PE_{thresh})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    $Q_{pr} = Q_{pr}/max(Q_{pr})$\\
    $Q_{pe} = Q_{pe}/min(Q_{pe})$\\
	\While{$True$}{
		$a=max_{arg}(Q_{pr})$\\
		\uIf{$Q_{pe}(a)+PE_{thresh}\geq max(Q_{pe})$}{
			break;
		}
		$Q_{pr}(a) = 0$
	}    
   
  
   \textbf{return} $a$
   
    
\caption{Action selection algorithm 4}
\end{algorithm}
\end{center}
\section{Neural network characteristics}
As for the characteristics of the neural network that is used. We started with a relatively small neural network and then grew it progressively as the number and size of the features was expanded. In the final version we use a fully connected neural network with four hidden layers of two hundred nodes each. They have each thirteen inputs, one for each feature and thirty-six outputs, one for each action. The first three layers use the Relu activation function and the last one uses a linear activation, since the rewards can be either negative or positive. We use stochastic gradient descent for updating the weights of the nodes and clip its norm to 1. We do this in order to avoid the exploding gradients problem and prevent the predicted values to diverge towards infinity. To calculate the loss, the mean squared error is used. The characteristics of the neural networks are based on related work as these types of networks seem to be the most widely used ones for deep reinforcement learning. It is worth noting that we decided not to do much parameter optimisation on the neural networks as the training time for the algorithm is quite long which would make parameter optimisation a very tedious task. It is also worth noting that we used the same network models for both performance and precision prediction.
\section{Training}

The training of the algorithm was done in multiple stages. During the first stage, only random actions were picked. This was done in order for the algorithm not to get stuck in local maxima but have the most global policy possible. We also did not have too large time constraints about the length of training so we could afford to proceed in this way in order to get the best possible results. Throughout the next stages, the training was separated into two. One concentrating on the precision and the other on the performance. All the actions were no longer chosen randomly, but now the action with the highest predicted value was chosen. The probability with which we chose a random action was progressively decreased throughout training. This was done in order to reinforce the choice of the best action and increase convergence speed. Once these stages finished, we would have two separate systems each optimised for their own task. In the final stage, we would then combine both of the systems and do a final training phase with the chosen action selecting algorithm as described above. We did this in order to further optimise the decision policy for the particular action selection algorithm.\\
There were two major complications that came up during training, that had to be dealt with. The first of these was that, especially during random training the likelihood of getting stuck on some join and not finishing the analysis of a benchmark was quite high. To confront this we set a timeout of thirty minutes on each benchmark. The goal of this is to find a compromise between having enough time to get to interesting joins and not wasting too much time in case the analysis gets stuck on a certain join. The other problem was that the analysis is very sensitive. That is to say, that one bad action can have severe consequences on the end precision of the analysis. This means that training with a low probability of picking random actions was essential for the estimators and had to be given enough time, but not too much so that we would avoid overfitting.\\
This method of training is quite time intensive. We decided to proceed in this way since time was not a big factor but the end results were more important. A more efficient training strategy can surely be found if this is needed.



















