% !TEX root = ../thesis.tex

\chapter{Fast polyhedra analysis with deep Q-networks}
\mbox{}\\
The goal of this work was to build upon the existing solutions used to make polyhedra analysis more efficient and use deep Q-networks to further improve the performance. Specifically, the goal was to extend the Elina Framework, more precisely the version of it that uses reinforcement learning for polyhedra analysis, in order to replace the Q-learning method with deep Q-networks for the Q-function estimation. Our work can be separated into three major subtasks. First of all, we had to figure out a way of calling a deep neural network from Elina, so that it could be used for the Q-function estimation. Secondly, In order to test out the validity of deep Q-networks on polyhedra analysis and to have a baseline for further optimisation, we implemented the basic deep Q-network training algorithm. Finally, we attempted to further optimise this problem by using task-specific knowledge.

\section{Incorporating Neural Networks inside Elina}
The first part, whilst definitely being the least interesting, was possibly the most laborious and a very key part of the whole work. The problem was that for the Q-value estimation we wanted to use neural network regression from the Keras framework in Python. We opted for this framework as it is very widely used for this type of problem, is relatively easy and intuitive to use whilst remaining very powerful. However, Elina, being written in C, the link between the python and C code had to be made. This was finally achieved by using the Python/C API and embedding the python code into the Elina framework.\\
One advantage of such a modification to the framework is that we can now learn online. In comparison to \cite{singh2018fast} where the training was done offline. Online training allows the algorithm to pick the best action according to the current decision policy. This is an essential feature for deep Q-networks, as it allows us to set an exploration rate and to reinforce the already learned policy, which is vital to prevent divergence.
\section{Basic algorithm}
In order to get the basic training algorithm running, first, the domain of polyhedra analysis had to be made compatible with the domain of reinforcement learning. As we have discussed in chapter 3.3.1., in order for reinforcement learning to work the following list of items has to be initialised inside the polyhedra domain:
\begin{itemize}
    \item A set of features describing the polyhedra, to use as states $S$
    \item A set of actions $A$
    \item A reward function $r$
\end{itemize}
Luckily, since a reinforcement learning method \cite{singh2018fast} has already been used for polyhedra analysis we could simply reuse what has already been developed for our baseline algorithm.\\
\subsection{Features}
The reinforcement learning method uses a set of nine different features.
\begin{itemize}
    \item The number of blocks.
    \item The minimal, maximal and average size of a block.
    \item The minimal, maximal and average size of the generator set of the union of input factors corresponding to a block.
    \item The number of variables with finite upper and lower bound.
    \item The number of variables with one finite and one infinite bound.
\end{itemize}
We will start off by reusing the same features. However, we will modify their bucketing, since, due to the nature of the Q-learning method, it is very restrictive. The new bucketing will be discussed in section 4.3.2.

\subsection{Actions}
As we discussed in section 3.2.1., the bottleneck of the decomposed analysis is the join. Therefore, the set of actions will be composed of various joins of different precision and performance.\\
First of all, the cost of the joins depends on the size of the blocks. Therefore, bounding this size with a threshold would increase the performance. These four different thresholds are used: $threshold \in [5,9],[10,14],[15,19],[20,\infty)$\\
Once we have decided on the size of a threshold, we have to determine what to do if the block has a greater size than the threshold. Different possibilities exist for splitting a large block into smaller ones, nonetheless, in essence, one has to pick a subset of constraints to remove from the block so as to make two subsets of its variables independent for it to be further partitionable. The following three constraint removals are used:
\subsubsection{Stoer-Wagner min-cut}
The first technique uses an algorithm based on the simple idea of removing the minimal amount of constraints in order to be able to split the block into two separate permissible partitions \cite{stoer1997simple}.
\subsubsection{Weighted constraint removal}
The second and third constraint removal techniques are based on the same principle. They associate a certain weight to each constraint and then remove the constraint with the highest one. Two different weight distribution techniques are considered. \\
In the first one, we compute for each variable $x_i \in X$ the number of constraints it appears in, we can call this $n_i$. Then, for each constraint $c_i$, we set its weight to the sum of $n_i$ of all the variables that are in the constraint.\\
In the second method, we first compute for each pair of variables $x_i,x_j\in X$, $n_{ij}$ the number of constraints containing both $x_i$ and $x_j$. The weight of the constraint is then the sum of all the $n_{ij}$ of all the pairs of variables $x_i,x_j$ contained in the constraint.\\
The idea behind removing the constraint with maximal weight is that most likely the variables with a large weight also occur in other constraints and therefore will not become unbounded once this constraint is removed. The precedent allows us to retain precision.
\subsubsection{Merging blocks}
The next three actions consist of various block merging strategies. The idea is to select different blocks and merge them together as long as the resulting blocks remain below the upper threshold limit, in order to increase the precision of the subsequent join. The following three block merging strategies are used:
\begin{itemize}
    \item No merge, no blocks are merged
    \item Merge smallest first, we first merge the smallest two blocks together. We then remove the smallest block and continue as long as the resulting merge remains below the threshold.
    \item Merge small with large, similar to the previous strategy but this time we merge the smallest block with the largest.
\end{itemize}

In total, we have four different thresholds, three different constraint removal algorithms and three different block merging strategies. We can mix and match these together as we please, which means that in total we have $4\cdot 3\cdot 3 =36$ different actions we can pick from.

\subsection{Reward}
As a reminder, the objective of the reward is to guide our learning policy, rewarding it when it takes actions towards our global goal and penalising it when it does otherwise. Therefore, the reward developed by the Q-learning method was the following:
\begin{equation}
    r(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb} - \log_{10}(cyc)
\end{equation}
Where $n_s$ is the number of variables with a singleton interval (i.e $x_i \in [a,b], a==b$) inside the resulting polyhedron after the join. $n_b$ is the number of bounded variables and $n_{hb}$ the number of semi-bounded variables of the resulting polyhedron. $cyc$ is the number of cycles required to perform the join.
\subsection{Back to deep Q-networks}
As we have finally initialised polyhedra analysis for reinforcement learning, we can proceed to the design of the training algorithm. Under its most basic form, the training algorithm could simply be the following. Pick a random action or the action with the maximal predicted Q-function, observe the reward and then retrain the network with the observed reward plus the discounted future reward on the given action. However, this very basic algorithm does not work very well as it runs into two major complications.

\paragraph{Divergence of decision policy}
The first of which is the divergence of the action selection policy. Meaning that the neural networks are not capable of creating a consistent policy, but rather pick different actions almost randomly most of the time.
\paragraph{Divergence of the action value prediction}
The second problem is that the values of the predictions from the estimators would diverge towards infinity. This is also known as the exploding gradient problem.\\
\mbox{}\\
These problems are not new and are a fundamental problem when trying to use nonlinear functions for the Q-function approximation. Several techniques exist to combat these problems, many of these reduce the complexity of the network and/or the problem in general. However, new methods have been developed recently that allow the training of the estimators without limiting the size of the neural network, the number of possible actions and allow the training to be done online. We decided the use the following two concepts inside of our algorithm.
\subsection{Experience replay memory}
Experience replay memory \cite{Mnih2015} is a biologically inspired mechanism. Its goal is to give the estimator a very basic concept of a memory and instead of directly learning from the current events happening, the agent learns from a random subset of its memory. More formally, during training, an array of a certain size filled with the past memory objects is kept. Each memory object contains the following items: $mem(s_t, a_t, r_t, s_{t+1})$. The current state, the action taken at this state, the observed reward and the next state. For training, we then simply pick a random subsample from the memory array and train from this data.
\\
The objective of the memory is to introduce more variety into the training data. It arises from the observation that during the execution of a program, a particular strategy would be optimal for a certain period of time and afterwards another one would be the new optimal. This causes two major problems. Firstly, it is not very time efficient as we do not gain much information by learning from the same data. Secondly, as there is low diversity in the training data, the decision strategy would often change abruptly. This either leads to the neural network getting stuck in local minima or simply to diverge and not obtain a policy. \\
Picking a random subsample from memory helps increase the variance amongst the training data allowing the network to learn a more global policy. 

\subsection{Separating target from max Q estimators}
The other method used to reduce the divergence of the predicted Q-values towards infinity was to reduce the correlation between the training and the prediction data \cite{Mnih2015}. This is done because, since the networks are being fitted partly on the maximum Q-value estimation, diverging towards infinity reduces their error and therefore is a valid strategy that the networks can exploit. In order to reduce this correlation, the networks predicting the maximum Q-value and the ones predicting the Q-value can be separated. The weights of the maximum Q-value estimators are then updated every n steps in order for them to remain up to date.\\
\mbox{}\\
Further modifications were also made on the neural network level in order to reduce these problems. We will further discuss these in section 4.4.\\
Once these modifications have been made, the basic training algorithm has the following form.
%\paragraph{Pseudo code of basic learning algorithm}

\begin{center}
\scalebox{1}{
\begin{minipage}{1.\linewidth}
\begin{algorithm}[H]
	\setstretch{0.7}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{learn} $(S,A,r,\gamma,\alpha,\phi,N,l\_freq,b\_size,\varepsilon,update\_nn\_freq)$\;
    
    
    \Indp\Input{\\$S \leftarrow states, A \leftarrow Actions, r \leftarrow reward,$\\$ \gamma \leftarrow discount$ $factor, \alpha \leftarrow learning$ $rate,$\\$\phi \leftarrow$ set of feature functions over S and A \\ N $\leftarrow$ size of memory, l\_freq$\leftarrow$ learning frequency,\\ b\_size $\leftarrow$ batch size, $\varepsilon \leftarrow$ random action probability,\\ update\_nn\_freq $\leftarrow$ frequency of updating max Q estimators}\Indm
    \Indp\Output{\\$\theta \leftarrow$ trained weights of the estimator\\}\Indm
    \Indp
    $\theta = $ initialise random weights and learning rate $\alpha$\\
    $\theta_{max} = $ $\theta$\\
    $M = $ initialise an empty memory stack\\
	\Indm
    \Indp\For{each episode}{
    start from initial state $s_{_0} \in S$\\
    \For{$t=0,1,2,...$}{
    Initialise new memory item $m_t$\\
    With prob. $\varepsilon$: $a_t= rand(1,36)$, with prob. $(1- \varepsilon)$: $a_t = max_{arg}(Q(:,s_t))$\\
	observe next state $s_{t+1}$ and
     $r_{t}(s_{t},a_t,s_{t+1})$\\
     Set $m_t.a = a_t,m_t.s_1=s_{t},m_t.s_{2}=s_{t+1},m_t.r=r_{t}$ \\
     
     \Indm
     \Indp\uIf{$M_{size} \geq N$}{
    	del $M_0$ \\
     	$M_{N}=m_t$
		}
		\uElse{
		push $m_t$ on M
	}\Indm
	\Indp\uIf{$t\mod l\_freq = 0$}{
	select a random batch of size b\_size from M\\
	compute $Q(:,s_t)$ estimation with $\theta$\\
	compute $Q(:,s_{t+1})$ estimation with $\theta_{max}$\\
	set $Q(a,s_t) = Q(a,s_t) + \gamma*max(Q(:,s_{t+1}))$\\
	Fit weights $\theta$ with new training data
	}\Indm
	\Indp\uIf{$t\mod update\_nn\_freq = 0$}{
	set $\theta_{max} = \theta$
	}
			
    }
    
   }
   \textbf{return} $\theta$
    
\caption{Basic DQN Training algorithm}
\end{algorithm}
\end{minipage}%
}
\end{center}

\section{Deep Q-networks for faster Polyhedra analysis}
Once the basic version of the deep Q-network algorithm was designed we were able to train it, test it and obtain our first set of results. The discussion of these will be done in chapter 5. These results can from now on be used as a baseline and we will attempt to further improve them.\\
In the search for a more efficient algorithm, we decided to modify the training algorithm with the use of some problem-specific knowledge, as well as further optimisations to the feature vector and reward functions. The modifications made will be discussed in the rest of this chapter.


\subsection{Separating the problem into two}
The first problem specific modification was dividing the problem into two independent subproblems. Fundamentally, the objective of Polyhedra Analysis is to obtain the most precise result in the quickest way possible. These are two independent objectives. Our goal was, therefore, to separate these two tasks and create two independent subsystems, one focusing on making the results as precise as possible, and the other on the time complexity of getting there.\\
 Such a division of the problem offers two main advantages. Firstly, we obtain a greater flexibility during training. The parameters of each system can be tuned for its specific needs, for example, the characteristics of the neural network or the discount factor. Furthermore, the optimal features used for precision and performance estimation are most likely different. The separation, therefore, allows us to only use the necessary features for each subsystem making the learning and prediction less complex and therefore more precise and converge faster. The reward as well can be fine-tuned for the needs of the specific subsystem.\\ 
 Secondly, two separate subsystems also allow for a more flexible algorithm post training. Since during training, we will have two different Q-function estimators, during prediction we will have a greater possibility of optimising our results by changing the importance we give to each subsystem at different points in time according to the specific needs. We will get into more detail about this in section 4.3.4.\\
 The pseudo code for the training algorithm with separated estimators has the following form:


\begin{center}
	
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{learn} $(S,A,r,\gamma,\alpha,\phi,len,l\_freq,b\_size,\varepsilon,update\_nn\_freq,SA)$\;
    
    
    \Indp\Input{\\$S \leftarrow states, A \leftarrow Actions, r \leftarrow reward,$\\$ \gamma \leftarrow discount$ $factor, \alpha \leftarrow learning$ $rate,$\\$\phi \leftarrow$ set of feature functions over S and A \\ len $\leftarrow$ size of memory, l\_freq$\leftarrow$ learning frequency,\\ b\_size $\leftarrow$ batch size, $\varepsilon \leftarrow$ random action probability,\\ update\_nn\_freq $\leftarrow$ frequency of updating max Q estimators,\\ SA $\leftarrow$ action selection algorithm}\Indm
    \Indp\Output{\\$\theta_1 \leftarrow$ trained weights of neural network for performance \\ $\theta_2 \leftarrow$ trained weights of neural network for precision}\Indm
    \mbox{}\\
    \Indp
    $\theta_1 = $ initialise random weights and learning rate $\alpha$\\
    $\theta_2 = $ initialise random weights and learning rate $\alpha$\\
    $\theta_{1\_max} = $ $\theta_1$\\
    $\theta_{2\_max} = $ $\theta_2$\\
    $M = $ initialise an empty memory stack\\
    
    \For{each episode}{
    start with initial states $s_{pr\_0} \in S, s_{pe\_0} \in S$\\
    \For{$t=0,1,2,...$}{
    Initialise new memory item $m_t$\\
    With prob. $\varepsilon$: $a_t= rand(1,36)$, prob. $(1- \varepsilon)$: $a_t = max_{arg}(SA(Q_{1,2}(:,s_t)))$\\  
    Observe next state $s_{pr\_t+1},s_{pe\_t+1}$ and
     $r_{pe}(s_{pe\_t},a_t,s_{pe\_t+1}),r_{pr}(s_{pr\_t},a_t,s_{pr\_t+1})$\\
     Set $m_t.a = a_t,m_t.s_{pr\_1}=s_{pr\_t},m_t.s_{pe\_1}=s_{pe\_t},m_t.s_{pr\_2}=s_{pr\_t+1},m_t.s_{pe\_2}=s_{pe\_t+1},m_t.r_{pr}=r_{pr},m_t.r_{pe}=r_{pe}$ \\
     
     \Indm
     \Indp\uIf{$M_{size} \geq len$}{
    	del $m_0$ \\
     	$M_{len}=m_t$
		}
		\uElse{
		push $m_t$ on M
	}\Indm
	\Indp\uIf{$t\mod l\_freq = 0$}{
	select a random batch of size b\_size from m\\
	compute $Q_{1,2}(:,s_t)$ estimation with $\theta_1,\theta_2$\\
	compute $Q_{1,2}(:,s_{t+1})$ estimation with $\theta_{1\_max},\theta_{2\_max}$\\
	set $Q_{1,2}(a,s_t) = Q_{1,2}(a,s_t) + \gamma*max(Q_{1,2}(:,s_{t+1}))$\\
	Fit weights $\theta_1,\theta_2$ with new data from this batch
	}\Indm
	\Indp\uIf{$t\mod update\_nn\_freq = 0$}{
	set $\theta_{1\_max} = \theta_1,\theta_{2\_max}= \theta_2$
	}
			
    }
    
   }
   \textbf{return} $\theta_1,\theta_2$
    
\caption{Separated DQN Training algorithm}
\end{algorithm}
\end{center}



\subsection{Feature selection}
%Before we get into the specific features that were used for the estimators, let's go a bit more in-depth about the theory of features and states. In the case of polyhedra analysis, its states would be the concrete Polyhedra involved. However, during our analysis, we do not use the concrete polyhedra as this would be too complicated, but rather information that we extract from them, we collect this information inside of the feature vector. As explained in section 2.1., for reinforcement learning, states should respect the Markov property, which states that state $s_{t+1}$ only depends on $s_t$ and on $a_t$. Whilst this is the case for the polyhedra, our feature vectors do not necessarily obey this property as they do not represent the polyhedra themselves but only some information about them. The more precisely we manage to describe our polyhedron the closer we will be to respecting the Markov property and the better will be the results of reinforcement learning. Therefore adding new features increases our description exactness of the polyhedron and makes our feature vector respect the Markov property more closely, this should hopefully increase the capabilities of our RL algorithm.
The goal of our algorithm is to be able to choose the correct abstractions on the polyhedra so that their join will be efficient and precise. For this to be possible, the algorithm has to have a concrete knowledge about the polyhedra being analysed. It gets this knowledge through the feature vector. Therefore, in order to maximise the precision of our algorithm, we want to describe the polyhedra as closely as possible, so that our algorithm can make its decisions precisely. There are two main ways of increasing the precision of our feature vector, by adding new features or by making the existing features more precise. However, it is important not to expand this too much, or the algorithm will no longer be able to select the relevant information and the decision policy will diverge.\\
With regards to the old set of features. One big inconvenience of using Q-learning is that, since we want to represent our Q-function with basis functions, the size and dimensions of our feature vector are very limited. This inconvenience is a lot less important when using methods such as deep Q-networks, as the input to these can be continuous. This allowed us to make two major changes. 
\subsubsection{Adding new features}
First of all, we reused the nine features from the Q-learning algorithm. As a reminder, these features are the following: the first seven features are used to characterise the complexity of the join. They are the number of blocks, minimal, maximal and average size of the blocks and the minimal, maximal and average size of the generator set. The last two features are used to characterise the precision of the inputs and they are the number of variables with a finite upper and lower bound, as well as the number of variables with a finite upper or lower bound, in both Polyhedra.\\
We further extended this set with four new features in order to further increase the description accuracy of the feature vector. The selection of these features was done by trial and error, with an experimental observation of an increase, or lack thereof, of accuracy. It is also possible to check whether a particular feature is useful once training is finished by analysing the neural network and observing the impact a particular feature has on the decision policy. However, since at the end our network has a total of four layers, this made its analysis somewhat complicated.\\
In the end, we decided to add a total of four new features, three of which are used for modelling the precision and one for the complexity. For the complexity, we added the number of variables. As for the precision, we added the number of unconstrained variables, the number of variables with a singleton interval and finally, the sum of the values the bounded variables can have (i.e. an approximation of the circumference of the polyhedron). We also attempted to use features that would have perhaps modelled the precision more accurately, such as most notably approximating the volume of the polyhedra. Unfortunately, one must also consider the complexity of computing the features. The computation of the volume approximation was far too time consuming, which greatly increased the learning time making the feature not viable.
\subsubsection{Bucketing}
Due to the limitations of Q-learning the old algorithm uses a very restrictive bucketing policy. Deep reinforcement learning does not suffer from such restrictions. It was, therefore, possible for us to remove the bucketing from the feature selection. We still decided to implement a version of bucketing inside of the algorithm for three main reasons. \\
\begin{itemize}
	\item For features with very big values, an exact precision is not needed. For example, for the feature that approximates the circumference of the polyhedron, its total value can be very big and if it is a million and one or just a million does not impact the resulting precision much, therefore bucketing does not impact it either.
	\item Bucketing greatly decreases the learning time and helps convergence.
	\item Bucketing allows to control the impact of the different features on the decision policy. More precisely, we wanted to ensure that the said decision policy does not give more importance to some features simply because they are larger in value than others. In total, we have thirteen different features and they can all have very different values. For example, the number of blocks varies mainly between one and ten and the circumference of the polyhedra can go up to $10^9$. This does not mean that the circumference has a greater impact on the overall description of the polyhedron. Bucketing assures that all features have a similar impact on the decision policy.
\end{itemize}
In order to decide the values of the bucketing that would be used,  the algorithm was run on a big number of benchmarks computing the maximal, minimal and average values the different features could have. Finally, they were scaled in such a way, so that they would all approximately remain between the bounds of zero and ten.\\
We describe the final features in the following table. The first column describes what the particular feature represents. The second, the complexity of extracting it. The third, an approximate range of values the feature can have during a normal execution. Finally, the last column describes how we scale our features. $round(x/y,z)$ means that we first divide the feature by $y$ and then round it to the nearest $z$.



\clearpage
\begin{center}
\renewcommand{\arraystretch}{1.3}
\captionof{table}{Features used in the DQN algorithms}
\Indm\Indm\begin{tabular}{||c c c c||} 
 
 \hline
 Feature & Extraction complexity & Approximate range & Scaling \\ [0.5ex] 
 \hline\hline
 $|\beta|$ & $O(1)$ & 1-10 & $x$ \\ 
 \hline
 $min(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $max(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $avg(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $min(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $max(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $avg(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $\{|x_i \in X: x_i \in (-\infty,\infty)$ in $P_m|\}$ & $O(ng)$ & 1-100 & $round((x/10),0.5)$ \\ 
  \hline
 $\{|x_i \in X: x_i \in [l_m,\infty)$ in $P_m|\} + $ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
 $ \{|x_i \in X: x_i \in (-\infty,u_m]$ in $P_m|\} $ & & & \\
 \hline 
 $\{|x_i \in X: x_i \in [l_m,u_m]$ in $P_m|\}$ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
   \hline
 $\{|x_i \in X: x_i \in [u_m,u_m]$ in $P_m|\}$ & $O(ng)$ & 1-50 & $round((x/20),0.2)$ \\ 
    \hline
 $|X|$ & $O(|\beta|)$ & 1-200 & $round((x/5),0.5)$ \\ 
    \hline
 $\sum (u_m-l_m):u_m,l_m \in\{[l_m,u_m]$ in $P_m \}$ & $O(ng)$ & $2*10^9$ & $round((x/2*10^8),0.01)$ \\ 
 
 
 \hline
\end{tabular}
\end{center}
One thing to note is that opposed to the reinforcement learning algorithm, these features do not have a maximal possible value. We believe that this increases the precision of the Q-function estimation, especially for the approximation of the complexity. Certain features can have very large values and we believe that when this happens, it has a major impact on the complexity of the join. However, these very large values were ignored by the bucketing of the Q-learning algorithm. 

\subsection{Reward Function modelling}
Due to the nature of the new algorithm, the modelling of the reward function was also separated into two subproblems. Firstly, the reward for the precision estimator and then the reward for the performance estimator.
\subsubsection{Performance reward}
Modelling the complexity is fairly simple and very straightforward, as simply counting the number of cycles needed for the computer to execute the join perfectly models the joins complexity and is exactly what we want to optimise. One thing to note is that we want this reward to be as small as possible, two simple solutions for this are either to invert it or to negate it. After some experimentation with both of these, we decided that negating it produces better results. This is probably due to the fact, that inverting the reward makes it have a nonlinear curve and its derivative loses importance the higher the CPU cycles are, which is not something we want to model. Another thing to note is that modelling the complexity reward in this way gives us another advantage over the reinforcement learning version of the algorithm. The Q-learning algorithm took the $\log_{10}$ off the CPU Cycles in order for the complexity and the precision reward to have similar values. We believe that this produces a similar problem as inverting the reward. The reward is no longer linear and its differences lose importance the higher it gets. Once again, this is not something that we want to model. The final rewards have the following form:
\begin{equation}
	r_{pe}(s_t,a_t,s_{t+1}) = -1 \cdot cyc
\end{equation}
Where $cyc$ is the number of CPU cycles needed to perform the join.
\subsubsection{Precision reward}
As to the second reward, modelling the precision of the resulting join. This is considerably more difficult than modelling the complexity as there is no trivial element giving us the precision of our resulting polyhedron. In order to choose the reward, we proceeded by intuitively picking a small set of options and verifying them experimentally. In the end, we tested out three different rewards.\\
We took the first one from the reinforcement learning algorithm, that is, its objective is to maximise the number of variables with a singleton interval, bounded and half bounded variables. The second is a further extension by penalising the number of values a bounded variable can have in the resulting Polyhedron. The final reward function is slightly different. In this one, we penalise the loss of a variable with a singleton interval, as well as the loss of a bounded or half bounded variable and penalise the number of values a bounded variable can have. We take the logarithm of the amount of values a variable can have, as this values can be very big and we do not want it to have a higher influence on the reward than the other values. The final rewards have the following form:
\begin{equation}
		r_{pr_1}(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb}
\end{equation}
\begin{equation}
		r_{pr_2}(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb} - \log_{10}(|n_b|)
\end{equation}
\begin{equation}
		r_{pr_3}(s_t,a_t,s_{t+1}) = 3  \cdot (n_{s_f} - n_{s_i}) + 2 \cdot (n_{b_f} - n_{b_i}) + (n_{hb_f} - n_{hb_i}) - \log_{10}(|n_b|)
\end{equation}\mbox{}\\
Where $n_s$ is the number of variables with singleton interval inside the resulting polyhedron after the join. $n_b$ is the number of bounded variables and $n_{hb}$ the number of semi-bounded variables of the resulting polyhedron. $n_{x_f}$ is the number of variables after the join and $n_{x_i}$ before the join.
\subsection{Action selection algorithms}
As discussed before, the Q-function estimation was separated into two independent subproblems. As previously described, this allows a certain amount of benefits that would not be possible otherwise. However, it also imposes one major complication. These two subproblems are not totally separable, as once we have predicted the two Q-function estimations, we have to somehow merge the information contained in both of these predictions and pick an ideal action accordingly. We tested several different action selection algorithms in order to find the one that maximises precision and performance the most. One thing to note is that it is at least partially possible to test out these algorithms post training. Simply put, we do not have to train using these in order to measure their performance afterwards. However, this can only work if we train in a purely random manner. If we were to train using one selection algorithm and then test with another, this would surely deteriorate the results for the other algorithm. The fact that we are able to test the selection algorithms after random training is still very helpful as the training time is relatively high and having to train for each algorithm would be very time intensive.
\subsubsection{Algorithm 1}
The first selection algorithm we used is perhaps the most intuitive one. First, we scale both predictions, the performance one between [-1,0] and the precision one between [0,1]. We then add the values together and pick the action with the maximal value.\\
Whilst seeming fair this type of selection has a couple of fundamental flaws. First of all, reinforcement learning estimates the best action to take in order to maximise the long-term objective, however, it has less of a guarantee towards the second and third to best actions. This means that if the network is well trained, the chances that the action with the maximal Q-value prediction is also the best action to take at this point in time should be quite high. However, the guarantee that the action with the second highest Q-value prediction is the second-best action is much smaller. Using this selection, the algorithm frequently chooses not the best action but the second best or the third etc... thus reducing the probability that they are also good actions. The second problem with this type of selection algorithm is that scaling the reward function causes a potentially important loss of information. Let's demonstrate this with an example, let's say that we have four joins j1,j2,j3,j4. J1 takes one second, j2 ten seconds, j3 one hour and j4 ten hours. Let's also say that j1 is more precise than j2 and j3 is more precise than j4. This algorithm is going to treat the difference between j1 and j2 the same as the one between j3 and j4, even though the gain in precision might be worth the loss in performance for the case of j1 and j2, whilst this would probably not be the case for j3 and j4.
\begin{center}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select1} $(Q_{pr},Q_{pe})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    $Q_{pr} = Q_{pr}/max(Q_{pr})$\\
    $Q_{pe} = Q_{pe}/min(Q_{pe})$\\
    $a = max_{arg}(Q_{pr}+Q_{pe})$
    
  
   \textbf{return} $a$

\caption{Action selection algorithm 1}
\end{algorithm}
\end{center}
\subsubsection{Algorithm 2}
The second algorithm used, that we designed specifically to confront both problems of the previous algorithm, proceeds as follows. We set a threshold for the performance. We then look at the action that has the maximal Q-value for precision. If this actions Q-value for performance is above the threshold we take it, otherwise we take the second-best action and continue until we find an action that is above the threshold.\\
Once again, this algorithm seems quite good at first glance and maybe, if executed perfectly, it would be the best option. Nevertheless and much like the previous one, it has some fundamental problems. First of all, picking a threshold is not a very simple task. The predictions are done with regression neural networks, the activation function of the last layer is linear, this means that the output values are unbounded and they do not have a direct correlation with Real-time CPU cycles. In order to pick a threshold, we had to experimentally try different possible values and observe the resulting precision and adjust accordingly, however, this threshold would vary according to the benchmark and therefore choosing an optimal one was a very difficult task on its own. Another problem of this algorithm is that if no action has a Q-performance estimation above the threshold the algorithm will choose the action with the worst precision, and this one does not even have to have the best performance estimation. 
\begin{center}
	\begin{algorithm}[H]
	\setstretch{0.75}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select2} $(Q_{pr},Q_{pe},PE_{thresh})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    \While{$max(Q_{pr})\neq 0$}{
    a = $max_{arg}(Q_{pr})$\\
    \uIf{$Q_{pe}(a)\geq PE_{thresh}$}{
    break;
    }
    $Q_{pr}(a)=0$
    }
    
   
  
   \textbf{return} $a$
   
    
\caption{Action selection algorithm 2}
\end{algorithm}
\end{center}
\subsubsection{Algorithm 3}
Another possibility is to slightly modify the previous algorithm in order to minimise its shortcomings. This time we set both a threshold for the performance and a certain number x. If the action that has the highest precision estimation is under the threshold for performance, we consider the x best actions according to their precision and take the one that has the highest performance estimation.\\
The problem of picking an appropriate threshold remains the same and this time we have the further problem of picking a correct value for x. However, the case that all actions are under the threshold is no longer a problem. It is worth noting that this algorithm also has a greater time complexity, however, this is still greatly outweighed if the correct action is taken.
\begin{center}
	\begin{algorithm}[H]
	\setstretch{0.75}

    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select3} $(Q_{pr},Q_{pe},PE_{thresh},N_{act})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold, \\$N_{act}\leftarrow$number of actions to consider if below threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
   	$a=max_{arg}(Q_{pr})$\\
   	\uIf{$Q_{pe}(a)\leq PE_{thresh}$}{
   		$a_{max}=a$\\
   		$val_{max}=Q_{pr}(a)$\\
   		$Q_{pr}(a)=0$\\
   		\For{$i \in N_{act}$}{
		   	$a=max_{arg}(Q_{pr})$\\
		   	\uIf{$Q_{pe}(a)\geq val_{max}$}{
		   		$a_{max}=a$\\
		   		$val_{max} = Q_{pe}(a)$
		   	}
		   	$Q_{pr}(a) = 0$
   		}
   	}
    
   
  
   \textbf{return} $a_{max}$
   
    
\caption{Action selection algorithm 3}
\end{algorithm}
\end{center}
\subsubsection{Algorithm 4}
Finally, the last selection algorithm that we will introduce is based on the previous one but altered in order to reduce the importance of choosing a correct parameter. This time we begin by scaling the performance prediction to [-1,0]. We set a threshold between [0,1]. We pick the action that has the highest Q-precision estimation. If the absolute value of this action's performance estimation minus the maximal performance estimation is below the threshold, then we pick this action, otherwise we pick the second-best precision action until we find one that is below the threshold.\\
This time, since we compare to the best performance action we are guaranteed to be below the threshold at some point, therefore, in the worst case, we will pick the action with the best performance. The threshold is also a lot easier to set as it is bounded. Unfortunately, once again we have the problem caused by the scaling of the performance estimation. However, after some experimental observation, we observed that when the join is fast, all the Q-performance estimations tend to be quite close together, which hopefully renders the problem inconsequential. 
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select4} $(Q_{pr},Q_{pe},PE_{thresh})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    $Q_{pe} = Q_{pe}/min(Q_{pe})$\\
	\While{$True$}{
		$a=max_{arg}(Q_{pr})$\\
		\uIf{$Q_{pe}(a)+PE_{thresh}\geq max(Q_{pe})$}{
			break;
		}
		$Q_{pr}(a) = 0$
	}    
   
  
   \textbf{return} $a$
   
    
\caption{Action selection algorithm 4}
\end{algorithm}
\end{center}
\section{Neural network characteristics}
As to the characteristics of the employed neural network. We started with a relatively small one and then grew it progressively as the number and size of the features were expanded. In the final version, we use a fully connected neural network with four hidden layers of two hundred nodes each. All of whom have thirteen inputs, one for each feature and thirty-six outputs, one for each action. The first three layers use the Relu activation function and the last one uses a linear activation since the rewards can be either negative or positive. We use stochastic gradient descent for updating the weights of the nodes and clip its norm to 1. We do this in order to avoid the exploding gradients problem and prevent the predicted values from diverging towards infinity. To calculate the loss, the mean squared error is used. The characteristics of the neural networks are based on other such projects, as these types of networks seem to be the most widely used ones for deep reinforcement learning. It is worth noting that we decided not to do much parameter optimisation on the neural networks as the training time for the algorithm is quite long which would make the parameter optimisation a very tedious task. In the end, we used the same network models for both performance and precision prediction as well as for the basic DQN algorithm.
\section{Training}

The algorithm was trained in multiple stages. During the first stage, only random actions were picked. This was done so that the algorithm would not get stuck in local minima but have the most global policy possible. We also did not have too large time constraints about the length of training so we could afford to proceed in this way in order to get the best possible results. Throughout the next stages, the training was separated into two. One concentrated on the precision and the other on the performance. All the actions were no longer chosen at random, instead, the one with the highest predicted value was selected. For this, we used the first action selection algorithm and scaled the values with a factor of two depending on the corresponding goal we were focusing on. The probability with which we chose a random action was progressively decreased throughout training. Thus, reinforcing the already learned strategy and increasing the convergence speed. Once these stages were complete, we would have two separate systems each optimised for their own task. In the final stage, we would then combine both of the systems and do a final training phase with the chosen action selecting algorithm as described above. We did this in order to further optimise the decision policy for the particular action selection algorithm.\\
During training, we encountered two major complications that we dealt with as follows. The first of these was that, especially during random training, the likelihood of getting stuck on a join and not finishing the analysis of a benchmark was quite high. To confront this we set a timeout of thirty minutes on each benchmark. This was done in the aim of finding a compromise between having enough time to get to interesting joins and not wasting too much time in case the analysis gets stuck on a certain join. The other problem was that the analysis is very sensitive. That is to say, one bad action can have severe consequences on the end precision of the analysis. This means that training with a high probability of exploitation was essential for the estimators and had to be given enough time, but not too much so that we would avoid overfitting.\\
This method of training is quite time intensive. We decided to proceed in this way since time was not a big factor but the end results were more important. A more efficient training strategy can surely be found if this is needed.



















