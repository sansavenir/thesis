% !TEX root = ../thesis.tex

\chapter{Fast polyhedra analysis with deep Q-networks}

The goal of my work was to build upon the existing solutions, used to make polyhedra analysis more efficient and use deep Q-networks to further improve the performance. Specifically, my goal was to extend the Elina Framework, the version of it that used reinforcement learning for polyhedra analysis, in order to use deep Q networks for the Q function estimation. The work I have done can be separated into three major subtasks. First of all, I had to figure out a way of calling a deep neural network form Elina, so that I could use it for the Q function estimation. Secondly, In order to test out the validity of deep Q-networks on polyhedra analysis and to have a baseline for further optimisation. I implemented the basic deep Q-network training algorithm. Finally, I attempted to further optimise this problem with some task specific knowledge.
\section{Incorporating Neural Networks inside Elina}
The first part, whilst definitely being the least interesting was probably the most laborious and a very key part of the whole work. The problem was that for my Q value estimation I wanted to use neural network regression from Keras in Python. I decided to use this framework as it is very widely used for this type of problem, is relatively easy and intuitive to use whilst remaining very powerful. However, Elina being written in C, I somehow had to make the link between my Python code and the C code of Elina. I finally achieved this by using the Python/C API and embedding the python code into the Elina framework. Initialising all the python code as well as importing all the necessary libraries such as Keras, is done in the $op\_pk\_manager\_alloc()$ function so that it doesn't have to be done every time we call the join function. Inside the join function we simply have to call the learning or prediction algorithms.
\section{Basic algorithm}
In order to get the basic training algorithm working, first the domain of polyhedra analysis has to be made compatible with the domain of reinforcement learning. Therefore the following list of items have to be initialised inside of the polyhedra domain:
\begin{itemize}
	\item A set of features describing the polyhedra to use as states $S$
	\item A set of actions $A$
	\item A reward function $r$
\end{itemize}
Luckily, since reinforcement learning methods have already been used for polyhedra analysis we can simply reuse what has already been developed for our baseline algorithm.\\
\subsection{Features}
In the Reinforcement learning method the following nine features was used.
\begin{center}

 \Indm\Indm\begin{tabular}{||c c c c||} 
 
 \hline
 Feature & Extraction complexity & Approximate range & Scaling \\ [0.5ex] 
 \hline\hline
 $|\beta|$ & $O(1)$ & 1-10 & $x/1.$ \\ 
 \hline
 $min(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $max(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $avg(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $min(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $max(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $avg(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $\{|x_i \in X: x_i \in [l_m,\infty)$ in $P_m|\} + $\\$ \{|x_i \in X: x_i \in (-\infty,u_m]$ in $P_m|\} $ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
  \hline
 $\{|x_i \in X: x_i \in [l_m,u_m]$ in $P_m|\}$ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
 
 \hline
\end{tabular}
\end{center}

\section{Actions}
As we discussed in 3.3.1., the bottleneck of the decomposed analysis is the join. Therefore, the set of actions will be a various set of joins of different precision and performance.\\
First of all, the cost of the joins depends on the size of the block. Therefore, bounding the the size of the block with a threshold would increase the performance. These four different thresholds are used $threshold \in [5,9],[10,14],[15,19],[20,\infty)$\\
Once we have decided on the size of a threshold we have to equally decide on what to do if the block has a greater size than the threshold. Their are different possibilities of how to split a large block into smaller ones, but basically, one has to pick a subset of constraints to remove from the block so that all its constraints are no longer dependant and then it can be further partitioned. The following three constraint removals are used:
\paragraph{Stoer-Wagner min-cut}\mbox{}\\
This uses the basic idea of removing the minimal amount of constraints in order to be able to split to block into two separate permissible partitions.
\paragraph{Weighted constraint removal}\mbox{}\\
The second and third constraint removal techniques are based on the same principal. They associate a certain weight to each constraint  and then remove the constraint with the highest weight. Two different weight distribution techniques are considered. \\
In the first one, we first compute for each variable $x_i \in X$ the number of constraints it appears in, we can call this $n_i$. Then for each constraint $c_i$ we set its weight to the sum of $n_i$ of all the variables that are in the constraint.\\
The second method, we first compute for each pair of variables $x_i,x_j\in X$, $n_{ij}$ the number of constraints containing both $x_i$ and $x_j$. The weight of the constraint is then the sum of all the $n_{ij}$ of all the pairs of constraints $x_i,x_j$ contained in the constraint.\\
The idea of removing the constraint with maximal weight, is that, most likely the variables occurring in the constraint are also bounded by other constraints and therefore will not become unbounded once the constraint is removed.
\paragraph{Merging blocks}\mbox{}\\
The next three actions are various block merging strategies. The idea is to select different blocks and merge them together as long as the resulting blocks remain below the threshold in order to increase the precision of the subsequent join. The following three block merging strategies are used:
\begin{itemize}
	\item No merge, no blocks are merged
	\item Merge smallest first, we first merge the smallest two blocks together. We then remove the smallest block and continue as long as the resulting merge remains below the threshold.
	\item Merge small with large, similar to the previous strategy but this time me merge the smallest block with the largest.
\end{itemize}

In total we have four different thresholds, three different constraint removal algorithms and three different block merging strategies. We can mix and match these together as we please, which means that in total we have $4\times 3\times 3 =36$ different actions we can pick.



Once this has been done, I could finally proceed to the more interesting part of writing the training algorithm. Under its most basic form, the training algorithm could simply be the following. Pick a random reward or maximal action, observe the reward and then retrain the network with the observed plus the discounted  future reward on the given action. This very basic algorithm however did not get me very far as I ran into two main problems.
\section{Reward}
As a reminder, the objective of the reward is to guide our learning policy. Rewarding it when it takes actions towards our global goal and penalising it when it does otherwise. Therefore, the reward developed by the Q-learning method was the following:
\begin{equation}
	r(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb} - \log_{10}(cyc)
\end{equation}
Where $n_s$ is the number of exactly defined variables of the resulting polyhedron after the join. $n_b$ is the number of bounded variables and $n_{hb}$ the number of semi-bounded variables of the resulting polyhedron. $cyc$ is the number of cycles required to perform the join.
\paragraph{Back to deep Q-networks}\mbox{}\\
As we have finally initialised polyhedra analysis for the use with reinforcement learning. We can finally proceed to the more interesting part of designing the training algorithm. Under its most basic form, the training algorithm could simply be the following. Pick a random reward or maximal action, observe the reward and then retrain the network with the observed plus the discounted  future reward on the given action. This very basic algorithm however did not get me very far as I ran into two main problems.

\paragraph{Divergence of decision policy}\mbox{}\\
The first of which was the divergence of the action selection strategy. Meaning that neural networks were not able of creating a consistent strategy but would be picking different actions all the time.
\paragraph{Divergence of the action value prediction}\mbox{}\\
The second problem was that the predictions of my estimator would diverge towards infinity. This is also known as the exploding gradient problem.\\
\mbox{}\\
These problems are not new and are a fundamental problem of trying to use non-linear functions for the Q-function approximation. Several techniques exist to combat these problem. Many of which reduce the complexity of the network and or the problem in general. However, some new methods have been developed recently that would allow the training of the estimators whilst retaining the problems original complexity. I decided the use the following two concepts inside of my algorithm.
\subsection{Experience replay memory}
Experience replay memory is a biologically inspired mechanism. That gives the estimator a very basic concept of a memory and instead of directly learning from the current events happening. The agent learns from a random subset of its memroy. More formally, during training, an array of a certain size filled with the past memory objects is kept. Each memory item contains the following items: $mem(s_t, a_t, r_t, s_{t+1})$. The current state, the action taken at this state, the observed reward and the next state. For training we then simply pick a random subsample from the memory array and train from these examples.
\\
The objective of the memory is to homogenise the training data. It comes from the observation that during the execution of a program, often a particular strategy would be optimal during a certain period of time and afterwards another one would be the new optimal. This caused two major problems. First of all, it is not very time efficient as we do not gain much information by learning from the same data. Secondly, as there was low diversity in the training data and the decision strategy would frequently abruptly change it. This either led to the neural network getting stuck in local minima or simply to diverge and not obtain a policy. \\
Picking a random subsample from memory helps increase the variance amongst the training data allowing the network to learn a more global policy. 
\subsection{Q-estimator separation}
The other method used to reduce the divergence of the predicted Q values towards infinity, was to reduce the correlation between the training and the prediction data. This was done because, since the networks were being fitted based on the maximum Q value estimation, diverging towards infinity reduced their error and therefore was a valid strategy that they would use. In order to reduce this correlation, the networks predicting the maximum Q value and the ones predicting the Q value were separated. The weights of the maximum Q-value estimators were then updated every n steps in order for them to remain up to date.\\
\mbox{}\\
Further modifications were also made on the neural network level in order to reduce these problems. I will get into these in part.\\
%TODO add part
Once these modifications have been made the basic training algorithm has the following form.
\paragraph{Pseudo code of basic learning algorithm}

\begin{center}
	
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{learn} $(S,A,r,\gamma,\alpha,\phi,N,l\_freq,b\_size,update\_nn\_freq,\varepsilon)$\;
    
    
    \Indp\Input{\\$S \leftarrow states, A \leftarrow Actions, r \leftarrow reward,$\\$ \gamma \leftarrow discount$ $factor, \alpha \leftarrow learning$ $rate,$\\$\phi \leftarrow$ set of feature functions over S and A \\ N $\leftarrow$ size of memory, l\_freq$\leftarrow$ learning frequency,\\ b\_size $\leftarrow$ batch size, $\varepsilon \leftarrow$ random action probability,\\ update\_nn\_freq $\leftarrow$ frequency of updating max Q estimators}\Indm
    \Indp\Output{\\$\theta \leftarrow$ trained weights of the estimator}\Indm
    \Indp
    $\theta = $ initialise random weights and learning rate $\alpha$\\
    $\theta_{max} = $ initialise random weights and learning rate $\alpha$\\
    $M = $ initialise an empty memory with size N\\
    
    \For{each episode}{
    start from initial state $s_{_0} \in S$\\
    \For{$t=0,1,2,...$}{
    Initialise new memory item $m_t$\\
    With probability $\varepsilon$ take random or maximal action $a_t$\\
	observe next state $s_{t+1}$ and
     $r_{t}(s_{t},a_t,s_{t+1})$\\
     Set $m_t.a = a_t,m_t.s_1=s_{t},m_t.s_{2}=s_{t+1},m_t.r=r_{t}$ \\
     
     \Indm
     \Indp\uIf{$M_{size} \geq N$}{
    	del $M_0$ \\
     	$M_{N}=m_t$
		}
		\uElse{
		push $m_t$ on M
	}\Indm
	\Indp\uIf{$t\mod l\_freq = 0$}{
	select a random batch of size b\_size from m\\
	compute $Q(:,s_t)$ estimation with $\theta$\\
	compute $Q(:,s_{t+1})$ estimation with $\theta_{max}$\\
	set $Q(a,s_t) = Q(a,s_t) + \gamma*max(Q(:,s_{t+1}))$\\
	Fit weights $\theta$ with new training data
	}\Indm
	\Indp\uIf{$t\mod update\_nn\_freq = 0$}{
	set $\theta_{max} = \theta$
	}
			
    }
    
   }
   \textbf{return} $\theta$
    
\caption{DQN Training algorithm}
\end{algorithm}
\end{center}
\section{Further optimisations}
The results of this algorithm will be discussed in the results section. I will now go into more details about further optimisations and different strategies that I tried in order to further improve the efficiency of the algorithm. I will now go into more detail about these modifications.

\subsection{Separating the problem into two}
The first problem specific modification I made, was subdividing the problem into two independent subproblems. The objective of Polyhedra Analysis is to obtain the most precise result in the quickest way possible. These are two independent objectives. My goal was therefore to separate these two tasks and create two independent subsystems, one focusing on making the results as precise as possible, and the other on the time complexity of getting there.\\
 Such a division of the problem, offer two main advantages. Firstly, a greater flexibility during training. The parameters of each system can be tuned for its specific needs. So for example the characteristics of the neural network or the discount factor. Furthermore, the optimal features used for precision and performance estimation, are quite probably different. The separation therefore allows us to use only the necessary features for each subsystem making the learning and prediction less complex and therefore more precise and converge faster. The reward as well can be fine tuned for the needs of the specific needs of the subsystem.\\ 
 Secondly, two separate subsystems also allow for a more flexible algorithm post training. Since during training, we will have trained two different Q function estimators, during prediction we will have a greater possibility of optimising our results by changing the importance we give to each subsystem at different points in time according to the specific needs. I will get into more detail about this in the section about the action selection policy.\\
 The pseudo code for the training algorithm with separated estimators has the following form:


\paragraph{Pseudo code of the separated training algorithm}
\begin{center}
	
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{learn} $(S,A,r,\gamma,\alpha,\phi,len,l\_freq,b\_size,update\_nn\_freq)$\;
    
    
    \Indp\Input{\\$S \leftarrow states, A \leftarrow Actions, r \leftarrow reward,$\\$ \gamma \leftarrow discount$ $factor, \alpha \leftarrow learning$ $rate,$\\$\phi \leftarrow$ set of feature functions over S and A \\ len $\leftarrow$ size of memory, l\_freq$\leftarrow$ learning frequency,\\ b\_size $\leftarrow$ batch size,\\ update\_nn\_freq $\leftarrow$ frequency of updating max Q estimators}\Indm
    \Indp\Output{\\$\theta_1 \leftarrow$ trained weights of neural network for performance \\ $\theta_2 \leftarrow$ trained weights of neural network for precision}\Indm
    \Indp
    $\theta_1 = $ initialise random weights\\
    $\theta_2 = $ initialise random weights\\
    $\theta_{1\_max} = $ initialise random weights\\
    $\theta_{2\_max} = $ initialise random weights\\
    $m = $ initialise an empty memory\\
    
    \For{each episode}{
    start with initial states $s_{pr\_0} \in S, s_{pe\_0} \in S$\\
    \For{$t=0,1,2,...$}{
    Initialise new memory item $m_t$\\
    Take action $a_t$ according to the action selection algorithm \\ observe next state $s_{pr\_t+1},s_{pe\_t+1}$ and
     $r_{pe}(s_{pe\_t},a_t,s_{pe\_t+1}),r_{pr}(s_{pr\_t},a_t,s_{pr\_t+1})$\\
     Set $m_t.a = a_t,m_t.s_{pr\_1}=s_{pr\_t},m_t.s_{pe\_1}=s_{pe\_t},m_t.s_{pr\_2}=s_{pr\_t+1},m_t.s_{pe\_2}=s_{pe\_t+1},m_t.r_{pr}=r_{pr},m_t.r_{pe}=r_{pe}$ \\
     
     \Indm
     \Indp\uIf{$m_{size} \geq len$}{
    	del $m_0$ \\
     	$m_{len}=m_t$
		}
		\uElse{
		push $m_t$ on m
	}\Indm
	\Indp\uIf{$t\mod l\_freq = 0$}{
	select a random batch of size b\_size from m\\
	compute $Q(:,s_t)$ estimation with $\theta_1,\theta_2$\\
	compute $Q(:,s_{t+1})$ estimation with $\theta_{1\_max},\theta_{2\_max}$\\
	set $Q(a,s_t) = Q(a,s_t) + \gamma*max(Q(:,s_{t+1}))$\\
	Fit weights $\theta_1,\theta_2$ with new computations from this batch
	}\Indm
	\Indp\uIf{$t\mod update\_nn\_freq = 0$}{
	set $\theta_{1\_max} = \theta_1,\theta_{2\_max}= \theta_2$
	}
			
    }
    
   }
   \textbf{return} $\theta_1,\theta_2$
    
\caption{DQN Training algorithm}
\end{algorithm}
\end{center}



\section{Feature selection}
Before I get into the specific features I used for my estimators, I would like to go a bit more in-depth about the theory of features and states. States in the case of polyhedra analysis would be the concrete Polyhedra. However, during our analysis we do not use the concrete polyhedra as this would be too complicated, but rather some information that we extract from them, we call this the feature vector. As explained in section 2.1., for reinforcement learning, states should respect the Markov property that states that $s_{t+1}$ only depends on $s_t$ and on $a_t$. Whilst this is the case for the polyhedra, our feature vectors do not have to obey this property as they do not represent the polyhedra themselves but only some information about them. The more precisely we manage to describe our polyhedron the closer we will be to respecting the Markov property and the better will be the results of RL. 

With regards to the old set of features. One big inconvenience of using Q-learning, is that, since we want to represent our Q-function with basis functions, the size and dimensions of our feature vector is very limited. This constraint no longer holds when using more advanced methods such as deep Q-networks. This allowed me to make two major changes about the feature vector. 
\paragraph{Adding new features}\mbox{}\\
The first change I was able to make, was to add some new features.First of all, I reused the nine features from the Q-learning algorithm. The first seven features of these features are used to characterise the complexity of the join. They are the number of blocks, minimal, maximal and average size of the blocks and the minimal, maximal and average size of the generator set. The last two features are used to characterise the precision of the inputs and they are the number of variables with a finite upper and lower bound, as well as the number of variables with a finite upper or lower bound, in both Polyhedra.\\
I further extended this set with four new features, in order to further increase the description accuracy of the feature vector. The selection of these features was done by trial and error, with an experimental observation of an increase, or lack thereof, off accuracy. It is also possible to check whether a particular feature is useful once training it finished by analysing the neural network and observing the impact a particular feature has on the end result. However, since at the end my network had a total of four layers, this made its analysis somewhat complicated.\\
At the end I added a total of four new features, three of which are used for modelling the precision and one for the complexity. For the complexity I added the number of constraints. As for the precision I added the number of unconstrained variables, the number of exactly constrained variables and finally, the sum of the values the bounded variables can have (i.e. an approximation of the circumference of the polyhedra). I also tried using some features that would have perhaps modelled the precision more accurately, such as most notably approximating the volume of the polyhedra. Unfortunately, one must also consider the complexity of computing the features. The computation of the volume approximation was far too time complex, which greatly increased the learning time making the feature not viable.\\
\paragraph{Bucketing}\mbox{}\\
Due to the limitations of Q-learning the old algorithm used a very restrictive bucketing policy. Deep reinforcement learning does not suffer from these restrictions. I was therefore able to remove the bucketing from the feature selecting. I still implemented a version of bucketing in my algorithm for three main reasons. \\
Firstly, for features with very big values an exact precision is not needed. For example, for the feature that approximates the circumference of the polyhedra, its total value can be very big and and if it is a million and one or just a million doesn't impact the resulting precision much therefore bucketing makes sense. \\
The second reason, for the use of bucketing is that it greatly decreases the learning time and helps convergence. \\
Finally, the last reason is that the decision policy doesn't give more importance to some features simply because they are larger than others. In my total of thirteen different features, they all can have very different values. For example, the number of blocks varies mainly between one and ten, and the circumference of the polyhedra can go up to $10^9$. This does not mean that the circumference has a greater impact on the overall precision of the polyhedra. Bucketing assures that all features have a similar impact on the decision policy.\\
In order to decide the values of the bucketing I would use, I ran my algorithm on a big number of benchmarks computing the maximal, minimal and average values the different features could have. Finally, I scaled them so that they would all approximately remain between the bounds of zero and ten. I then bucketed them in the following fashion:
\begin{center}

 \Indm\Indm\Indm\begin{tabular}{||c c c c||} 
 
 \hline
 Feature & Extraction complexity & Approximate range & Scaling \\ [0.5ex] 
 \hline\hline
 $|\beta|$ & $O(1)$ & 1-10 & $x/1.$ \\ 
 \hline
 $min(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $max(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $avg(|\chi_k|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-50 & $round((x/5),0.5)$ \\
 \hline
 $min(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $max(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $avg(|\bigcup G_{P_{m}}(\chi_k)|:\chi_k \in \beta)$ & $O(|\beta|)$ & 1-10000 & $round((x/1000),0.1)$ \\ 
 \hline
 $\{|x_i \in X: x_i \in (-\infty,\infty)$ in $P_m|\}$ & $O(ng)$ & 1-100 & $round((x/10),0.5)$ \\ 
 \hline
 $\{|x_i \in X: x_i \in [l_m,\infty)$ in $P_m|\} + $\\$ \{|x_i \in X: x_i \in (-\infty,u_m]$ in $P_m|\} $ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
  \hline
 $\{|x_i \in X: x_i \in [l_m,u_m]$ in $P_m|\}$ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
   \hline
 $\{|x_i \in X: x_i \in [u_m,u_m]$ in $P_m|\}$ & $O(1)$ & 1-200 & $round((x/20),0.2)$ \\ 
    \hline
 $|X|$ & $O(ng)$ & 1-50 & $round((x/5),0.5)$ \\ 
    \hline
 $\sum (u_m-l_m):u_m,l_m \in\{[l_m,u_m]$ in $P_m \}$ & $O(ng)$ & $2*10^9$ & $round((x/2*10^8),0.01)$ \\ 
 
 
 \hline
\end{tabular}
\end{center}
One thing to note is, that as opposed to the reinforcement learning algorithm, my features do not have a maximal possible value. I believe that this increases the precision of the q function estimation, especially for the approximation of the complexity. Certain features can have very large values and I believe when this arrives, it has a major impact on the complexity of the join. However, these very large values were ignored by the bucketing of the reinforcement learning algorithm. 

\section{Reward Function modelling}
Due to the nature of the new algorithm, the modelling of the reward function was separated into two separate subproblems.
\paragraph{Performance reward}\mbox{}\\
Firstly, the reward for the precision estimator and then the reward for the performance estimator. Modelling the complexity is fairly simple and very straight forward, as simply counting the number of cycles needed for the computer to execute the join perfectly models the complexity and is exactly what we want to optimise. One thing to note is that we want this reward to be as small as possible, two simple solutions for this are either to invert it or to negate it. I experimented with both of these and found that negating it produces better results. I assume this is due to the fact that inverting the reward, makes it have a nonlinear curve and its derivative looses importance the higher the CPU cycles are, which is not something we want. Another thing to note is that modelling the complexity reward in this way give us another advantage over the reinforcement learning version of the algorithm. The reinforcement learning algorithm took the $\log_{10}$ off the CPU Cycles in order for the complexity and the precision reward to have similar values. I believe that this produces a similar problem as inverting the reward. The reward is no longer linear and its differences loose importance the higher it gets. Once again, this is not something that we want to model. The final rewards has the following form:
\begin{equation}
	r_{pr_1}(s_t,a_t,s_{t+1}) = -1 \cdot cyc
\end{equation}
Where $cyc$ is the number of CPU cycles.
\paragraph{Precision reward}\mbox{}\\
As to the second reward, modelling the precision of the resulting join. This is considerably more difficult than modelling the complexity as there is no trivial element giving us the precision of our polyhedron. In order to choose the reward, I proceeded by intuitively picking a small set of options and verifying them experimentally at the end. At the end, I tested out three different reward.\\
I took the first one from the reinforcement learning algorithm, that is, the objective is to maximise the amount of exactly bounded, bounded and half bounded variables. The second is a further extension by penalising the amount of values a bounded variable can have in the resulting Polyhedra. The final, reward function is slightly different. In this one, I penalise the loss of an exactly bounded, bounded or half bounded variable. Reward the loss of a bounded variable and penalise the amount of values a bounded variable can have. I also tried basing a reward function on an approximation of the volume of the resulting Polyhedra, unfortunately, the complexity of this computation was too high which made it too impractical to use in practice. The final rewards have the following form:
\begin{equation}
		r_{pr_1}(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb}
\end{equation}
\begin{equation}
		r_{pr_2}(s_t,a_t,s_{t+1}) = 3  \cdot n_s + 2 \cdot n_b + n_{hb} - \log_{10}(|n_b|)
\end{equation}
\begin{equation}
		r_{pr_3}(s_t,a_t,s_{t+1}) = 3  \cdot (n_{s_f} - n_{s_i}) + 2 \cdot (n_{b_f} - n_{b_i}) + (n_{hb_f} - n_{hb_i}) - \log_{10}(|n_b|)
\end{equation}\mbox{}\\
Where $n_s$ is the number of exactly defined variables of the resulting polyhedron after the join. $n_b$ is the number of bounded variables and $n_{hb}$ the number of semi-bounded variables of the resulting polyhedron. $n_{x_f}$ is the number of variables after the join and $n_{x_i}$ before the join.
\section{Action selection algorithms}
As discussed before, I separated the Q function estimation into two separate subproblems. As I already mentioned earlier, this allows a certain amount of benefits that would not be possible otherwise. However, it also imposes one major complication. These two problems are not totally separable, as once we have predicted the two q function estimations, we have to somehow merge the information contained in both of these estimations and pick an ideal action accordingly. I tried out a few different action selection algorithms in order to find the one that maximises precision and performance the most. One thing to note is that it is at least partially possible to test out these algorithms post training. That is to say that we do not have to train using these in order to measure their performance afterwards. This only works if we train purely randomly however. As if, if we were to train using one selection algorithm and then test with another, this would surely deteriorate the results. The fact that we are able to test the selection algorithms after random training is still very helpful as the training time is relatively high and having to train for each algorithm would be very time intensive.
\paragraph{Algorithm 1}\mbox{}\\
The first selection algorithm I used is perhaps the most intuitive one. First scaling both q function estimations. The performance one between [-1,0] and the precision one between [0,1]. Adding the values together and picking the action with the maximal value. Whilst seeming very fair this type of selection has a couple of fundamental flaws. First of all, reinforcement learning estimates the best action to take in order to maximise the long-term objective, it however has less of a guarantee about the second to best and third to best action. This means that if the network is well trained, the chances that the action with the maximal Q-value prediction is also the best action to take at this point in time should be quite high. However, the guarantee that the action with the second highest Q-value prediction is the second-best action to take is much smaller. Using this selection algorithm tends to quite often not choose the best action but the second best or the third etc... making the probability that they are also good actions also a lot smaller. The second problem with this type of selection algorithm is that scaling the reward function causes a loss of information that could be very important. I will demonstrate this with an example, let's say we have four joins j1,j2,j3,j4. J1 takes one second, j2 ten seconds, j3 one hour and j4 ten hours. Let's also say that j1 and more precise than j2 and j3 is more precise than j5. This algorithm is going to treat the difference between j1 and j2 the same as the one between j3 and j4, even though the gain in precision might be worth to loss in performance for the case of j1 and j2, whilst this would probably not be case for j3 and j4.
\paragraph{Pseudo code of algorithm 1}
\begin{center}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select1} $(Q_{pr},Q_{pe})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    $Q_{pr} = Q_{pr}/max(Q_{pr})$\\
    $Q_{pe} = Q_{pe}/min(Q_{pe})$\\
    $a = max_{arg}(Q_{pr}+Q_{pe})$
    
  
   \textbf{return} $a$

\caption{Action selection algorithm 1}
\end{algorithm}
\end{center}
\paragraph{Algorithm 2}\mbox{}\\
The second algorithm used, that directly confront both problems of the previous algorithm proceeds as follows. We set some sort of threshold for the performance. We then look at the action that has the maximal Q-value for precision. If this actions Q-value for performance is above the threshold we take it otherwise we take the second-best action and continue until we find an action that is above the threshold. Again, this algorithm seems quite good at first glance and maybe if it was executed perfectly it would be the best option, but like the one before it has some fundamental problems. First of all, picking a threshold is not a very simple task. The q function estimator is a regression neural network, the activation function of its last layer is linear, this means that the output values are unbounded and they do not have a direct correlation with Realtime CPU cycles. In order to pick a threshold, I had to experimentally try different possible values and observe the resulting precision and adjust accordingly, however this threshold would vary according to the benchmark and therefore choosing an optimal one was a very difficult task on its own. Another problem of this algorithm is that If no action has a q performance estimation above the threshold the algorithm will choose the action with the worst precision, and this one does not even have to have the best performance estimation. 
\paragraph{Pseudo code of algorithm 2}
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select2} $(Q_{pr},Q_{pe},PE_{thresh})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    \While{$max(Q_{pr})\neq 0$}{
    a = $max_{arg}(Q_{pr})$\\
    \uIf{$Q_{pe}(a)\geq PE_{thresh}$}{
    break;
    }
    $Q_{pr}(a)=0$
    }
    
   
  
   \textbf{return} $a$
   
    
\caption{Action selection algorithm 2}
\end{algorithm}
\end{center}
\paragraph{Algorithm 3}\mbox{}\\
Another possibility is to slightly modify the previous algorithm in order to minimise some of its short comings. This time we set both some sort of a threshold for the performance and a certain number x. if the action that has the highest precision estimation is under the threshold for performance, we take the x best actions according to their precision and take the one that has the highest performance estimation. The problem of picking an appropriate threshold remains the same and this time we have the further problem of picking a correct value for x. However, the case that all actions are under the threshold is no longer a problem. It is worth noting that this algorithm also has a greater time complexity, however this is still greatly outweighed if the correct action is taken.
\paragraph{Pseudo code algorithm 3}
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select3} $(Q_{pr},Q_{pe},PE_{thresh},N_{act})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold, \\$N_{act}\leftarrow$number of actions to consider if below threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
   	$a=max_{arg}(Q_{pr})$\\
   	\uIf{$Q_{pe}(a)\leq PE_{thresh}$}{
   		$a_{max}=a$\\
   		$val_{max}=Q_{pr}(a)$\\
   		$Q_{pr}(a)=0$\\
   		\For{$i \in N_{act}$}{
		   	$a=max_{arg}(Q_{pr})$\\
		   	\uIf{$Q_{pe}(a)\geq val_{max}$}{
		   		$a_{max}=a$\\
		   		$val_{max} = Q_{pe}(a)$
		   	}
		   	$Q_{pr}(a) = 0$
   		}
   	}
    
   
  
   \textbf{return} $a_{max}$
   
    
\caption{Action selection algorithm 3}
\end{algorithm}
\end{center}
\paragraph{Algorithm 4}\mbox{}\\
Finally, the last selection algorithm that I will talk about here is based upon the last one and made in such a way as to reduce the importance of correct parameter choosing. This time we begin by scaling the performance prediction to [-1,0]. We set a threshold to [0,1]. We pick the action that has the highest precision q estimation. If the absolute value of this action's performance estimation minus the maximal performance estimation is below the threshold, then we pick this action otherwise we pick the second-best precision action until we find one that is below the threshold. This time, since we compare to the best performance action we are guaranteed to be below the threshold at some point, in the worst case we will pick the action with the best performance. The threshold is also a lot easier to set since it is bounded. Unfortunately, once again we have the problem caused by the scaling of the performance estimation. However, after some experimental observation, I saw that when the join is fast all the q performance estimations tend to be quite close together, so hopefully this will not be all too much of a problem.
\paragraph{Pseudo code algorithm 4}
\begin{center}
	\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \textbf{function} \text{select4} $(Q_{pr},Q_{pe},PE_{thresh})$\;
    
    
    \Indp\Input{\\$Q_{pr}\leftarrow$array of Q-function estimations for precision,\\$Q_{pr}\leftarrow$array of Q-function estimations for performance,\\ $PE_{thresh}\leftarrow$performance threshold}\Indm
    \Indp\Output{\\$a \leftarrow$ action to take} \Indm
    \Indp
    $Q_{pr} = Q_{pr}/max(Q_{pr})$\\
    $Q_{pe} = Q_{pe}/min(Q_{pe})$\\
	\While{$True$}{
		$a=max_{arg}(Q_{pr})$\\
		\uIf{$Q_{pe}(a)+PE_{thresh}\geq max(Q_{pe})$}{
			break;
		}
		$Q_{pr}(a) = 0$
	}    
   
  
   \textbf{return} $a$
   
    
\caption{Action selection algorithm 4}
\end{algorithm}
\end{center}
\section{Neural network characteristics}
As for the characteristics of the neural network that I used. I started with a relatively small neural network and the grew it progressively as I expanded the number of vectors and the size that these can have. In the final version I use a fully connected neural network with four hidden layers of two hundred nodes each. They have each thirteen inputs, one for each feature and thirty-six outputs, one for each action. The first three layers use the Relu activation function and the last one uses a linear activation, since the rewards can be either negative or positive. I use stochastic gradient descent for updating the weights of the nodes and I clip its norm to 1. I do this in order to avoid the exploding gradients problem and prevent the predicted values to diverge towards infinity. I use the mean squared error in order to calculate the loss. I based the characteristics of my neural networks based on reading as these types of networks seem to be the most widely used ones for deep reinforcement learning. It is worth noting that I have not done much parameter optimisation on my neural networks as the training time of the algorithm is quite long which would make the parameter optimisation a very tedious task. It is also worth noting that I use the same network models for both performance and precision prediction.
\section{Training}
\begin{itemize}
	\item getting stuck with random training
	\item one bad action would destroy the results
	\item 
\end{itemize}
The training of the algorithm was done in multiple stages. During the first stage, only random actions were picked. This was done in order for the algorithm not to get stuck in local maxima but have the most global policy possible. I also did not have any time constraints about the length of training so I could afford to proceed in this way in order to get the best possible results. Throughout the next stages, the training was separated into two. One concentrating on the precision and the other on the performance. All the actions were no longer chosen randomly, but now the action with the highest predicted value was chosen. The probability with which we chose a random action was progressively decreased throughout training. This was done in order to reinforce the choice of the best action and increase convergence speed. Once these stages finished, we would have two separate systems each optimized for their own task. In the final stage, I would then combine both of the systems and do a final training phase with the chosen action selecting algorithm as described above. I did this in order to further optimize the decision policy for the action selection algorithm.\\
This method of training is quite time intensive. I proceeded in this way since time was not a big factor but the end results were more important. A more efficient training strategy can surely be found if this is needed.
