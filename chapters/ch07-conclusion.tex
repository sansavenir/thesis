% !TEX root = ../thesis.tex

\chapter{Conclusion}

In conclusion, in this work we managed to build a framework for the training and testing of various deep reinforcement learning algorithms inside of polyhedra analysis. We were then able to design a series of such algorithms. These algorithms ranged from very global reinforcement learning methods, to ones more optimised with problem specific knowledge as well as having a greater flexibility towards the various subproblems. We then tested out the different subparts of our algorithms that we had come up with, in the goal of finding an optimal combination.\\
Once our algorithms were designed, we were then able to test them on a broad array of different benchmarks. They managed to outperform the other preexisting reinforcement learning methods on both accuracy and performance.\\
It is also worth noting that we managed to highlight the overall effectiveness of the global deep Q-network algorithm, as after all it was able to craft a decision policy that was the most accurate. When designing new training algorithms that used more domain specific knowledge, we were only able to increase the performance at the loss of some accuracy.\\
Future optimisations to this work could be done from several angles. Techniques such as CNN's could be investigated for automatic feature extraction from the polyhedra itself. Other works such as \cite{dyer1991random, kim2004fast} could equally be used for the approximation of the volume of the polyhedra. Both of these techniques could then be simultaneously used for expanding the features and the reward for the precision. Other than that, optimisations to the neural network itself could be undertook as this was not explored much in this work. An expansion of the action set, most notably the size and number of different thresholds would certainly improve the precision of the overall algorithm as well.
