% !TEX root = ../thesis.tex

\chapter{Conclusion}
%TODO continous action space for threshold
In conclusion, in this work we managed to build a framework for the training and testing of various deep reinforcement learning algorithms inside of polyhedra analysis. We were then able to design a series of such algorithms. These algorithms ranged from very global reinforcement learning methods, to ones more optimised with problem specific knowledge as well as having a greater flexibility towards the various subproblems. We then tested out the different subparts of our algorithms that we had come up with, in the goal of finding an optimal combination.\\
Once our algorithms were designed, we were then able to test them on a broad array of different benchmarks. They managed to outperform other preexisting reinforcement learning methods on both accuracy and performance.\\
It is also worth noting that we managed to highlight the overall effectiveness of the global deep Q-network algorithm, as after all it was able to craft a decision policy that was the most accurate. Even with the design of new training algorithms that used more domain specific knowledge, we were only able to increase the performance at the loss of some precision.\\
Future research for this work could be done from several directions. Techniques such as CNN's could be investigated for automatic feature extraction from the polyhedra itself. Other works such as \cite{dyer1991random, kim2004fast} could also be used for the approximation of the volume of the polyhedra. Both of these techniques could then be simultaneously used for expanding the features and the reward for precision. Other than that, optimisations to the neural network itself could be undertook as this was not explored much in this work. An expansion of the action set, most notably the size and number of different thresholds would certainly improve the precision of the overall algorithm as well. Finally, improving the features and neural networks and making these more task specific for precision or performance as well as trying new action selection strategies could surely also increase the overall results. 
