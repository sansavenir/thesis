% !TEX root = ../thesis.tex

% set counter to n-1:
\setcounter{chapter}{1}

\chapter{Reinforcement Learning}
\mbox{}\\
\mbox{}\\
\mbox{}\\
Reinforcement Learning \cite{kaelbling1996reinforcement} is a very general problem of machine learning studied in a multitude of different fields, such as game theory, information theory or statistics. It is a very broad concept and the foundations are the following. An agent interacts with a given environment, at its disposal, it has a set of various actions. When an action is performed it receives a reward. The goal of the agent is to devise an action selection policy that should maximise the cumulative reward of the selected actions. 

\begin{figure}[h]
	\begin{center}
		\includegraphics*[height=5cm]{figures/rl_broad.png}
		\caption{ Concepts of RL illustration \protect\cite{wiki:rl}}	
	\end{center}
\end{figure}

\section{Concepts}
In order to be able to solve a problem with reinforcement learning, it first has to be mapped to the following reinforcement learning (RL) concepts:
\begin{itemize}
	\item A set of agent states $S$, with the initial state $s_0\in S$
	\item A set of actions $A$
	\item A function giving the reward of performing an action from  $s_t$ to $s_{t+1}$, $r(s_t,a_t,s_{t+1})\in \mathbb{R}$
\end{itemize}
During the execution of the program, the agent will first start in the initial state $s_0$. Then, at each timestep $t=0,1,...,$ the agent will pick an action $a_t \in A$, this action will then be executed, the agent will move from $s_t$ to $s_{t+1}$ and receive the corresponding reward $r(s_t,a_t,s_{t+1})$. This process will be repeated until a final state is reached. In RL, state transitions typically satisfy the Markov property. This property states that the next state $s_{t+1}$ only depends on the current state $s_t$ and on the action taken at this state $a_t$. We call the sequence of actions and states from the initial to the final state an episode. The goal of the agent is to devise an action selection policy that maximises the cumulative reward, which is measured as the sum of the rewards inside of an episode. We will demonstrate this reward with the following example.
\paragraph{Example 2.1}\mbox{}\\
Let's assume we have an episode with two timesteps and an action set with only two possible actions. In the following tree, we can see the different actions that we can take at each state and their respective reward.\\
\begin{figure}[h]
\tikzstyle{level 1}=[level distance=3.5cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3.5cm, sibling distance=2cm]

% Define styles for bags and leafs
\tikzstyle{bag} = [text width=4em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]

\begin{center}
	
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {$s_0$}
    child {
        node[bag] {$s_1$}        
            child {
                node[end, label=right:
                    {$s_3,-5 + 10 = 5$}] {}
                edge from parent
                node[above] {$a_1$}
                node[below]  {$10$}
            }
            child {
                node[end, label=right:
                    {$s_4,-5 + 30 = 25$}] {}
                edge from parent
                node[above] {$a_0$}
                node[below]  {$30$}
            }
            edge from parent 
            node[above] {$a_1$}
            node[below]  {$-5$}
    }
    child {
        node[bag] {$s_2$}        
        child {
                node[end, label=right:
                    {$s_5,20 -2 = 18$}] {}
                edge from parent
                node[above] {$a_1$}
                node[below]  {$-2$}
            }
            child {
                node[end, label=right:
                    {$s_6, 20 + 3 = 23$}] {}
                edge from parent
                node[above] {$a_0$}
                node[below]  {$3$}
            }
        edge from parent         
            node[above] {$a_0$}
            node[below]  {$20$}
    };
\end{tikzpicture}
\caption{Example of an execution episode with two possible actions}
\end{center}
\end{figure}

The cumulative reward is the summed reward of all the actions taken during an episode. This is shown at the end of the leaf nodes in the example above. Therefore, an ideal decision policy would choose the actions $\{a_1,a_0\}$ in that order for the above episode.
 \section{Q-function}
 We call Q-function or quality function, a mapping $Q:S \times A \rightarrow \mathbb{R}$ that specifies the cumulative reward of picking an action $a_t$ in state $s_t$. If this function was known, the reinforcement learning problem would become quite trivial, as it would simply suffice to pick, at every time step, the action with the highest Q-function value.
 
 \section{Learning the Q-function} 
The Q-function, therefore, represents the ideal decision policy that one can make. However, since the Q-function is not known beforehand, the goal of reinforcement learning is to obtain it. Most RL approaches achieve this by iteratively updating their policy until a suitable one is learned, this is called training. During training, the agent is allowed to interact freely inside the environment, picking actions randomly or with the strategy learned so far. It observes the different rewards it obtains and updates its policy accordingly.\\ 
We shall now briefly introduce a few concepts used during training.
\paragraph{Exploration-exploitation tradeoff/dilemma}
As mentioned above, during training we either pick the best action with the till-now learned policy or we simply pick a random action. The trick is in finding a correct balance between the two, as each has its own particular advantage. This is called the exploration-exploitation dilemma \cite{yogeswaran2012reinforcement}. If we always pick the best action, we will converge quickly but the risk of getting stuck in local minima is fairly high. On the other hand, if we only pick random actions, the convergence will be very slow and we might simply diverge and not be able to learn a strategy at all. Most training algorithms start with a high probability of exploration and then, as training progresses, increase the exploitation probability in order to solidify the learned policy.
\paragraph{Learning rate}
The learning rate is also part of the exploration-exploitation dilemma. With this ratio, we can modify the importance that we give to newly acquired information. The lower it is the harder it will be to overwrite an already learned policy. With a high learning rate, we will adapt more easily to new problems, but this will be accompanied by a higher risk of overfitting.

\paragraph{Discount factor}
In reinforcement learning, we approximate the Q-function with the following equation:
\begin{equation}
	Q'(s,a) = \max_{\pi} \mathbb{E}[r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}+...| s_t=a, a_t = a,\pi]
\end{equation}
where $\pi$ is the learned policy.\\
We call $\gamma$ the discount factor. It represents the importance that we give to the future reward. If $\gamma = 0$ then only the immediate reward will be considered and if $\gamma \approx 1$ then we will learn a policy that will give the same importance to the future rewards as to the immediate reward. As before, one has to choose between a faster convergence and a higher risk of getting stuck in local minima and not learning a global policy.
\subsection{Q-learning}
Q-learning \cite{watkins1992q} is one of the main algorithms used in reinforcement learning. Given any Markov decision process, infinite time and a high exploration rate, Q-learning is guaranteed to learn the ideal decision policy for the particular problem. It works by iteratively updating its policy using a weighted average of the old values and the new information. Q-learning can obtain optimal results on simpler tasks. However, it lacks one key attribute.
\subsection{Generalisation}
Generalisation is the capacity of an agent to make decisions about unvisited states. As said before, Q-learning is guaranteed to learn the optimal policy, but only under the assumption that during training the agent will have explored all states multiple times and all actions will have been taken. If the state space of our problem was to be large, training in this way would be extremely time intensive. Therefore, a very interesting capacity for our agent would be the capability to generalise from trained data, towards previously unseen circumstances. Normal Q-learning lacks this capacity and if it is presented with a previously unvisited state, its policy will be no better than random.  In order for a generalisation to be possible, some kind of function approximation has to be done on the Q-function.
\subsection{Linear function approximation}
Several approximation techniques exist. One of the most popular ones, as well as an important technique for the rest of this paper,  is Q-learning with linear basis function approximation. This method models the Q-function as a linear combination of basis functions where each basis function assigns a value to a $(s_t \in S,a_t \in A)$ pair and each feature has its own basis function. Such an approximation offers several advantages: it is efficient, converges relatively quickly and the learned policy can be quite easily interpreted. Unfortunately, linear approximations also have three main shortcomings.
\begin{itemize}
    \item It requires a discrete state space
    \item It requires a discrete action space
    \item If the optimal policy is nonlinear, it cannot be learned
\end{itemize}
Some workarounds for these problems are possible, such as discretising the continuous variables. However, these also cause a loss of information and therefore precision.\\
Recently, a lot of work has been done on the incorporation of nonlinear function approximators inside of reinforcement learning. These methods do not suffer from the above-mentioned problems and have therefore been able to offer some very promising results on problems that were too complex for reinforcement learning in the past.
\subsection{Deep Q-networks}
Deep Q-networks (DQN) \cite{mnih2013playing} are a novel method used for reinforcement learning that has gained a lot of attention in recent years. This is due to the ability that a single algorithm has in achieving very good results on a broad array of tasks and this without the need of any specialised knowledge about the task beforehand. This has led some to consider the method as the first major step towards general artificial intelligence.\\
What separates deep Q-networks from other reinforcement learning methods is that they use neural networks (NN) (i.e nonlinear functions) to approximate the Q-function. Reinforcement learning has been known to diverge when nonlinear function approximators have been used. Various techniques have been developed to combat the divergence of these methods, such as neural fitted Q-iteration \cite{riedmiller2005neural} or experience replay memory \cite{Mnih2015}. These techniques have allowed NN's to become a viable tool for reinforcement learning and to achieve some very remarkable results. Most notably, \cite{Mnih2015} were able to design an algorithm that, without the need of any prior knowledge about the task, achieved human-level performance on a large number of different Atari games.

































